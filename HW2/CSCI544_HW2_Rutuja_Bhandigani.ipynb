{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa25e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074af79c",
   "metadata": {},
   "source": [
    "> **Task 1: Vocabulary Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de297a9d",
   "metadata": {},
   "source": [
    "In task 1, we first design a function to replace rare words whose occurence is less than the set threshold value with a special token <unk>. We use this function on the training data and store all unique words in a dataframe called vocab and arrange them in descending order of their occurence. Next, we find the rows which have the special token <unk> and move it to the top, while storing the index of each word in the vocab dataframe. Finally, we create the vocab.txt file which contains this vocabulary from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6528952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for replacing rare words: 2\n",
      "Size of vocabulary: 23183\n",
      "The total occurences of the special token <unk>: 20011\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train\", sep = \"\\t\", names = ['id', 'words', 'pos'])\n",
    "df['occ'] = df.groupby('words')[\"words\"].transform('size')\n",
    "\n",
    "threshold = 2\n",
    "def word_replace(row):\n",
    "    if row.occ < threshold:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return row.words\n",
    "\n",
    "df['words'] = df.apply(lambda row : word_replace(row), axis = 1)\n",
    "vocab = df.words.value_counts().rename_axis('words').reset_index(name = 'occ')\n",
    "\n",
    "unk = vocab[vocab['words'] == \"<unk>\"]\n",
    "\n",
    "index = vocab[vocab.words == \"<unk>\"].index\n",
    "vocab = vocab.drop(index)\n",
    "vocab = pd.concat([unk, vocab]).reset_index(drop = True)\n",
    "vocab['id'] = vocab.index + 1\n",
    "cols = vocab.columns.tolist()\n",
    "cols = [cols[0], cols[-1], cols[1]]\n",
    "vocab = vocab[cols]\n",
    "\n",
    "unk_count = int(vocab[vocab[\"words\"] == \"<unk>\"].occ)\n",
    "\n",
    "vocab.to_csv(\"vocab.txt\", sep=\"\\t\", header=None)\n",
    "\n",
    "print(\"Threshold for replacing rare words:\", threshold)\n",
    "print(\"Size of vocabulary:\",vocab.shape[0])\n",
    "print(\"The total occurences of the special token <unk>:\", unk_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491eedea",
   "metadata": {},
   "source": [
    "Preprocessing the train data into required format for the upcoming tasks. First, we collect all the unique tags from the train data. Then, we create a nested list for storing the training data where each list has a tuple corresponding to a row in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49cc85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df.pos.value_counts().rename_axis('pos').reset_index(name = 'count')\n",
    "tags = pos.pos.tolist()\n",
    "\n",
    "sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for line in df.itertuples():\n",
    "    if(line.id == 1 and first == 0):\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append((line.words, line.pos))\n",
    "sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af857974",
   "metadata": {},
   "source": [
    "> **Task 2: Model Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2b709",
   "metadata": {},
   "source": [
    "In task 2, trans_matrix function computes the transition matrix for the training data using the given formula, and emission_matrix function computes the emission matrix for the given sentences in traing data.\n",
    "trans_prob function converts the transition matrix into a transition dictionary, and emission_prob function converts the emission matrix into an emission dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134f845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_matrix(sentences, tags):\n",
    "    tag_cnt = {}\n",
    "    t_matrix = np.zeros((len(tags),len(tags)))\n",
    "    for tag in range(len(tags)):\n",
    "        tag_cnt[tag] = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            tag_cnt[tags.index(sentence[i][1])] += 1\n",
    "            if i == 0: \n",
    "                continue\n",
    "            t_matrix[tags.index(sentence[i - 1][1])][tags.index(sentence[i][1])] += 1\n",
    "    \n",
    "    for i in range(t_matrix.shape[0]):\n",
    "        for j in range(t_matrix.shape[1]):\n",
    "            if(t_matrix[i][j] == 0) : t_matrix[i][j] = 1e-10\n",
    "            else: t_matrix[i][j] /= tag_cnt[i]\n",
    "\n",
    "    return t_matrix\n",
    "\n",
    "\n",
    "def emission_matrix(tags, vocab, sentences):\n",
    "    tag_cnt = {}\n",
    "    e_matrix = np.zeros((len(tags), len(vocab)))\n",
    "    for tag in range(len(tags)):\n",
    "        tag_cnt[tag] = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, pos in sentence:\n",
    "            tag_cnt[tags.index(pos)] +=1\n",
    "            e_matrix[tags.index(pos)][vocab.index(word)] += 1\n",
    "\n",
    "    for i in range(e_matrix.shape[0]):\n",
    "        for j in range(e_matrix.shape[1]):\n",
    "            if(e_matrix[i][j] == 0) : e_matrix[i][j] = 1e-10\n",
    "            else: e_matrix[i][j] /= tag_cnt[i]\n",
    "\n",
    "    return e_matrix\n",
    "\n",
    "vocab = vocab.words.tolist()\n",
    "\n",
    "\n",
    "def trans_prob(tags, t_matrix, prior_prob):\n",
    "    tags_dict = {}\n",
    "\n",
    "    for i, tags in enumerate(tags):\n",
    "        tags_dict[i] = tags\n",
    "\n",
    "    trans_prob = {}\n",
    "    for i in range(t_matrix.shape[0]):\n",
    "        trans_prob['(' + '<\\S>' + ',' + tags_dict[i] + ')'] = prior_prob[tags_dict[i]]\n",
    "    for i in range(t_matrix.shape[0]):\n",
    "        for j in range(t_matrix.shape[1]):\n",
    "            trans_prob['(' + tags_dict[i] + ',' + tags_dict[j] + ')'] = t_matrix[i][j]\n",
    "\n",
    "\n",
    "    return trans_prob\n",
    "\n",
    "def emission_prob(tags, vocab, e_matrix):\n",
    "    tags_dict = {}\n",
    "\n",
    "    for i, tags in enumerate(tags):\n",
    "        tags_dict[i] = tags\n",
    "\n",
    "    emission_prob = {}\n",
    "\n",
    "    for i in range(e_matrix.shape[0]):\n",
    "        for j in range(e_matrix.shape[1]):\n",
    "            emission_prob['(' + tags_dict[i] + ', ' + vocab[j] + ')'] = e_matrix[i][j]\n",
    "\n",
    "    return emission_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd54d6",
   "metadata": {},
   "source": [
    "Here, get_all_prob function generates the transition matrix, emission matrix, transition dictionary and the emission dictionary, inital_prob function calculates the initial transition probability for each tag and we store the transition and emission dictionaries in a json file named 'hmm.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8e60c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Transition Parameters: 2070\n",
      "Number of Emission Parameters: 1043235\n"
     ]
    }
   ],
   "source": [
    "def get_all_prob(tags, vocab, sentences, prior_prob):\n",
    "    t_matrix = trans_matrix(sentences, tags)\n",
    "    e_matrix = emission_matrix(tags, vocab, sentences)\n",
    "                \n",
    "    transition_probability = trans_prob(tags, t_matrix, prior_prob)\n",
    "    emission_probability = emission_prob(tags, vocab, e_matrix)\n",
    "\n",
    "    return transition_probability, emission_probability\n",
    "\n",
    "def inital_prob(df, tags):\n",
    "    tags_start_cnt = {}\n",
    "    total_start_sum = 0\n",
    "    for tag in tags:\n",
    "        tags_start_cnt[tag] = 0\n",
    "    \n",
    "    for line in df.itertuples():\n",
    "        if(line[1] == 1):\n",
    "            tags_start_cnt[line[3]]+=1\n",
    "            total_start_sum += 1\n",
    "    \n",
    "    prior_prob = {}\n",
    "    for key in tags_start_cnt:\n",
    "        prior_prob[key] = tags_start_cnt[key] / total_start_sum\n",
    "    \n",
    "    return prior_prob\n",
    "\n",
    "prior_prob = inital_prob(df, tags)\n",
    "trans_prob, emission_prob = get_all_prob(tags, vocab, sentences, prior_prob)\n",
    "\n",
    "\n",
    "print(\"Number of Transition Parameters:\",len(trans_prob))\n",
    "print(\"Number of Emission Parameters:\",len(emission_prob))\n",
    "\n",
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump({\"transition\": trans_prob, \"emission\": emission_prob}, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ccf68a",
   "metadata": {},
   "source": [
    "> **Task 3: Greedy Decoding with HMM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2f599",
   "metadata": {},
   "source": [
    "In task 3, we read the dev file and create a nested list for storing the development data where each list has a tuple corresponding to a row in dev data<br>\n",
    "Implement greedy_decoding function which computes the state sequence for HMM Model using the greedy decoding technique<br>\n",
    "evaluate function computes the accuracy of the model model by comparing the the predicted tag sequence with groundtruth<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed70c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv(\"data/dev\", sep = '\\t', names = ['id', 'words', 'pos'])\n",
    "dev_data['occ'] = dev_data.groupby('words')['words'].transform('size')\n",
    "\n",
    "valid_sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for line in dev_data.itertuples():\n",
    "    if(line.id == 1 and first == 0):\n",
    "        valid_sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append((line.words, line.pos))\n",
    "valid_sentences.append(sentence)\n",
    "\n",
    "def greedy_decoding(trans_prob, emission_prob, prior_prob, valid_sentences, tags):\n",
    "    sequences = []\n",
    "    total_score = []\n",
    "    for sentence in valid_sentences:\n",
    "        prev_tag = None\n",
    "        sequence = []\n",
    "        score = []\n",
    "        for i in range(len(sentence)):\n",
    "            best_score = -1\n",
    "            for j in range(len(tags)):\n",
    "                state_score = 1\n",
    "                if i == 0:\n",
    "                    state_score *= prior_prob[tags[j]]\n",
    "                else:\n",
    "                    if str(\"(\" + prev_tag  + \",\" + tags[j] + \")\") in trans_prob:\n",
    "                        state_score *= trans_prob[\"(\" + prev_tag  + \",\" + tags[j] + \")\"]\n",
    "                \n",
    "                if str(\"(\" + tags[j] + \", \" + sentence[i][0] + \")\") in emission_prob:\n",
    "                    state_score *= emission_prob[\"(\" + tags[j] + \", \" + sentence[i][0] + \")\"]\n",
    "                else:\n",
    "                    state_score *= emission_prob[\"(\" + tags[j] + \", \" + \"<unk>\" + \")\"]\n",
    "                \n",
    "                if(state_score > best_score):\n",
    "                    best_score = state_score\n",
    "                    highest_prob_tag = tags[j]\n",
    "                    \n",
    "            prev_tag = highest_prob_tag\n",
    "            sequence.append(prev_tag)\n",
    "            score.append(best_score)\n",
    "        sequences.append(sequence)\n",
    "        total_score.append(score)\n",
    "\n",
    "    return sequences, total_score\n",
    "\n",
    "sequences, total_score = greedy_decoding(trans_prob, emission_prob, prior_prob, valid_sentences, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e4e7d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Greedy Decoding HMM model on dev data: 93.51132293121243\n"
     ]
    }
   ],
   "source": [
    "def evaluate(sequences, valid_sentences):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(len(valid_sentences)):\n",
    "        for j in range(len(valid_sentences[i])):\n",
    "\n",
    "            if(sequences[i][j] == valid_sentences[i][j][1]):\n",
    "                correct += 1\n",
    "            total +=1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print('Accuracy of Greedy Decoding HMM model on dev data: {}'.format(evaluate(sequences, valid_sentences)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f192c",
   "metadata": {},
   "source": [
    "Here, we read the test data and create a nested list for storing the testing data where each list has a tuple corresponding to a row in test data<br>\n",
    "The output_file function stores the predictions of pos tags using greedy decoding by the HMM model on the test data in 'greedy.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ce7fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test\", sep = '\\t', names = ['id', 'words'])\n",
    "test_data['occ'] = test_data.groupby('words')['words'].transform('size')\n",
    "test_data['words'] = test_data.apply(lambda row : word_replace(row), axis = 1)\n",
    "\n",
    "test_sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for line in test_data.itertuples():\n",
    "    if(line.id == 1 and first == 0):\n",
    "        test_sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append(line.words)\n",
    "test_sentences.append(sentence)\n",
    "\n",
    "test_sequences, test_score = greedy_decoding(trans_prob, emission_prob, prior_prob, test_sentences, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aacdf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_file(test_inputs, test_outputs, filename):\n",
    "    result = []\n",
    "    for i in range(len(test_inputs)):\n",
    "        s = []\n",
    "        for j in range(len(test_inputs[i])):\n",
    "            s.append((str(j+1), test_inputs[i][j], test_outputs[i][j]))\n",
    "        result.append(s)\n",
    "    \n",
    "    with open(filename + \".out\", 'w') as f:\n",
    "        for element in result:\n",
    "            f.write(\"\\n\".join([str(item[0]) + \"\\t\" + item[1] + \"\\t\" + item[2] for item in element]))\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "output_file(test_sentences, test_sequences, \"greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02dc518",
   "metadata": {},
   "source": [
    "> **Task 4: Viterbi Decoding with HMM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30776780",
   "metadata": {},
   "source": [
    "In task 4, we implement the viterbi decoding algorithm using viterbi_decoding function on dev data which computes the probabilty for each word in a sentence having a tag from the group of all tags<br>\n",
    "viterbi_backward function finds the best possible tag sequence for each sentence based on the probabilites calculated by the viterbi_decoding function<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3e81617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Viterbi Decoding HMM model on dev data: 94.80905834496994\n"
     ]
    }
   ],
   "source": [
    "def viterbi_decoding(trans_prob, emission_prob, prior_prob, sentence, tags):\n",
    "\n",
    "    n = len(tags)\n",
    "    viterbi_list = []\n",
    "    data = {}\n",
    "    for t in tags:\n",
    "        if str(\"(\" + t + \", \" + sentence[0][0] + \")\") in emission_prob:\n",
    "            viterbi_list.append(prior_prob[t] * emission_prob[\"(\" + t + \", \" + sentence[0][0] + \")\"])\n",
    "        else:\n",
    "            viterbi_list.append(prior_prob[t] * emission_prob[\"(\" + t + \", \" + \"<unk>\" + \")\"])\n",
    "\n",
    "    for i, l in enumerate(sentence):\n",
    "        word = l[0]\n",
    "        if i == 0: continue\n",
    "        temp_list = [None] * n\n",
    "        for j,tag in enumerate(tags):\n",
    "            score = -1\n",
    "            val = 1\n",
    "            for k, prob in enumerate(viterbi_list):\n",
    "                if str(\"(\" + tags[k] + \",\" + tag + \")\") in trans_prob and str(\"(\" + tag + \", \" + word + \")\") in emission_prob:\n",
    "                    val = prob * trans_prob[\"(\" + tags[k] + \",\" + tag + \")\"] * emission_prob[\"(\" + tag + \", \" + word + \")\"]\n",
    "                else:                   \n",
    "                    val = prob * trans_prob[\"(\" + tags[k] + \",\" + tag + \")\"] * emission_prob[\"(\" + tag + \", \" + \"<unk>\" + \")\"]\n",
    "                if(score < val):\n",
    "                    score = val\n",
    "                    data[str(i) + \", \" + tag] = [tags[k], val]\n",
    "            temp_list[j] = score\n",
    "        viterbi_list = [x for x in temp_list]\n",
    "    \n",
    "    return data, viterbi_list\n",
    "\n",
    "\n",
    "c = []\n",
    "v = []\n",
    "for sentence in valid_sentences:\n",
    "    a, b = viterbi_decoding(trans_prob, emission_prob, prior_prob, sentence, tags)\n",
    "    c.append(a)\n",
    "    v.append(b)\n",
    "\n",
    "def viterbi_backward(tags, data, viterbi_list):\n",
    "\n",
    "    num_states = len(tags)\n",
    "    n = len(data) // num_states\n",
    "    best_sequence = []\n",
    "    best_sequence_breakdown = []\n",
    "    x = tags[np.argmax(np.asarray(viterbi_list))]\n",
    "    best_sequence.append(x)\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        val = data[str(i) + ', ' + x][1]\n",
    "        x = data[str(i) + ', ' + x][0]\n",
    "        best_sequence = [x] + best_sequence\n",
    "        best_sequence_breakdown =  [val] + best_sequence_breakdown\n",
    "    \n",
    "    return best_sequence, best_sequence_breakdown\n",
    "\n",
    "best_seq = []\n",
    "best_seq_score = []\n",
    "for data, viterbi_list in zip(c, v):\n",
    "\n",
    "    a, b = viterbi_backward(tags, data, viterbi_list)\n",
    "    best_seq.append(a)\n",
    "    best_seq_score.append(b)\n",
    "\n",
    "print('Accuracy of Viterbi Decoding HMM model on dev data: {}'.format(evaluate(best_seq, valid_sentences)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd6546",
   "metadata": {},
   "source": [
    "Here, we use the viterbi decoding algorithm for predicting the pos tags of all sentences in the test data <br>\n",
    "We use the output_file function to store the predictions of Viterbi decoding by the HMM model on the test data in 'viterbi.out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2ab8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [] \n",
    "v = []\n",
    "for sentence in test_sentences:\n",
    "    a, b = viterbi_decoding(trans_prob, emission_prob, prior_prob, sentence, tags) \n",
    "    c.append(a)\n",
    "    v.append(b)\n",
    "    \n",
    "best_seq = []\n",
    "best_seq_score = []\n",
    "for data, viterbi_list in zip(c, v):\n",
    "    a, b = viterbi_backward(tags, data, viterbi_list)\n",
    "    best_seq.append(a)\n",
    "    best_seq_score.append(b)\n",
    "output_file(test_sentences, best_seq, 'viterbi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

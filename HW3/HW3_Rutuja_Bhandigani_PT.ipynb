{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "45fc2f49",
      "metadata": {
        "id": "45fc2f49"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import contractions\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader \n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ddfb93f",
      "metadata": {
        "id": "4ddfb93f"
      },
      "outputs": [],
      "source": [
        "#! pip install bs4 # in case you don't have it installed\n",
        "#! pip install contractions\n",
        "\n",
        "#! pip install gensim\n",
        "#gensim version 4.3.0\n",
        "#!pip install --upgrade gensim\n",
        "\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('omw-1.4')\n",
        "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Dataset Generation"
      ],
      "metadata": {
        "id": "OKtHGN3G78vT"
      },
      "id": "OKtHGN3G78vT"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d8d05d21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "d8d05d21",
        "outputId": "5749c79b-5582-4573-d830-5fc88db994d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
              "0          US      1797882  R3I2DHQBR577SS  B001ANOOOE         2102612   \n",
              "1          US     18381298  R1QNE9NQFJC2Y4  B0016J22EQ       106393691   \n",
              "2          US     19242472  R3LIDG2Q4LJBAO  B00HU6UQAG       375449471   \n",
              "3          US     19551372  R3KSZHPAEVPEAL  B002HWS7RM       255651889   \n",
              "4          US     14802407   RAI2OIG50KZ43  B00SM99KWU       116158747   \n",
              "\n",
              "                                       product_title product_category  \\\n",
              "0  The Naked Bee Vitmin C Moisturizing Sunscreen ...           Beauty   \n",
              "1      Alba Botanica Sunless Tanning Lotion, 4 Ounce           Beauty   \n",
              "2          Elysee Infusion Skin Therapy Elixir, 2oz.           Beauty   \n",
              "3  Diane D722 Color, Perm And Conditioner Process...           Beauty   \n",
              "4  Biore UV Aqua Rich Watery Essence SPF50+/PA+++...           Beauty   \n",
              "\n",
              "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
              "0           5            0.0          0.0    N                 Y   \n",
              "1           5            0.0          0.0    N                 Y   \n",
              "2           5            0.0          0.0    N                 Y   \n",
              "3           5            0.0          0.0    N                 Y   \n",
              "4           5            0.0          0.0    N                 Y   \n",
              "\n",
              "                                     review_headline  \\\n",
              "0                                         Five Stars   \n",
              "1                          Thank you Alba Bontanica!   \n",
              "2                                         Five Stars   \n",
              "3                                         GOOD DEAL!   \n",
              "4  this soaks in quick and provides a nice base f...   \n",
              "\n",
              "                                         review_body review_date  \n",
              "0                   Love this, excellent sun block!!  2015-08-31  \n",
              "1  The great thing about this cream is that it do...  2015-08-31  \n",
              "2  Great Product, I'm 65 years old and this is al...  2015-08-31  \n",
              "3  I use them as shower caps & conditioning caps....  2015-08-31  \n",
              "4  This is my go-to daily sunblock. It leaves no ...  2015-08-31  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-033770be-8d10-4ec9-a634-fecfa7aa26b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>marketplace</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_parent</th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US</td>\n",
              "      <td>1797882</td>\n",
              "      <td>R3I2DHQBR577SS</td>\n",
              "      <td>B001ANOOOE</td>\n",
              "      <td>2102612</td>\n",
              "      <td>The Naked Bee Vitmin C Moisturizing Sunscreen ...</td>\n",
              "      <td>Beauty</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Love this, excellent sun block!!</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>18381298</td>\n",
              "      <td>R1QNE9NQFJC2Y4</td>\n",
              "      <td>B0016J22EQ</td>\n",
              "      <td>106393691</td>\n",
              "      <td>Alba Botanica Sunless Tanning Lotion, 4 Ounce</td>\n",
              "      <td>Beauty</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Thank you Alba Bontanica!</td>\n",
              "      <td>The great thing about this cream is that it do...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US</td>\n",
              "      <td>19242472</td>\n",
              "      <td>R3LIDG2Q4LJBAO</td>\n",
              "      <td>B00HU6UQAG</td>\n",
              "      <td>375449471</td>\n",
              "      <td>Elysee Infusion Skin Therapy Elixir, 2oz.</td>\n",
              "      <td>Beauty</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US</td>\n",
              "      <td>19551372</td>\n",
              "      <td>R3KSZHPAEVPEAL</td>\n",
              "      <td>B002HWS7RM</td>\n",
              "      <td>255651889</td>\n",
              "      <td>Diane D722 Color, Perm And Conditioner Process...</td>\n",
              "      <td>Beauty</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>GOOD DEAL!</td>\n",
              "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US</td>\n",
              "      <td>14802407</td>\n",
              "      <td>RAI2OIG50KZ43</td>\n",
              "      <td>B00SM99KWU</td>\n",
              "      <td>116158747</td>\n",
              "      <td>Biore UV Aqua Rich Watery Essence SPF50+/PA+++...</td>\n",
              "      <td>Beauty</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>this soaks in quick and provides a nice base f...</td>\n",
              "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-033770be-8d10-4ec9-a634-fecfa7aa26b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-033770be-8d10-4ec9-a634-fecfa7aa26b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-033770be-8d10-4ec9-a634-fecfa7aa26b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "data = pd.read_csv(\"amazon_reviews_us_Beauty_v1_00.tsv\", sep = '\\t', on_bad_lines='skip')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ed0de025",
      "metadata": {
        "id": "ed0de025"
      },
      "outputs": [],
      "source": [
        "data = data[[\"review_body\", \"star_rating\"]]\n",
        "data.dropna(inplace = True)\n",
        "data = data.astype({'star_rating': 'int'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a16f0fd6",
      "metadata": {
        "id": "a16f0fd6"
      },
      "outputs": [],
      "source": [
        "# Create a new column 'class' based on the 'star_rating' column\n",
        "data['class'] = data['star_rating'].apply(lambda x: 1 if x in [1, 2] else 2 if x == 3 else 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2ad45990",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2ad45990",
        "outputId": "28ab4c53-6b34-4b50-fe6e-ba39cd40bc74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    20000\n",
              "2    20000\n",
              "3    20000\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data_class_1 = data[data['class'] == 1].sample(n=20000, random_state=1)\n",
        "data_class_2 = data[data['class'] == 2].sample(n=20000, random_state=1)\n",
        "data_class_3 = data[data['class'] == 3].sample(n=20000, random_state=1)\n",
        "\n",
        "# Concatenate the resulting dataframes to create a balanced dataset\n",
        "data = pd.concat([data_class_1, data_class_2, data_class_3])\n",
        "data['class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "212c710c",
      "metadata": {
        "id": "212c710c"
      },
      "outputs": [],
      "source": [
        "# print average length of reviews before cleaning\n",
        "data['review_length'] = data['review_body'].str.len()\n",
        "review_len_before_cleaning = data['review_length'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d4db95b3",
      "metadata": {
        "id": "d4db95b3"
      },
      "outputs": [],
      "source": [
        "# Convert all reviews to lowercase\n",
        "data['review_body'] = data['review_body'].str.lower()\n",
        "\n",
        "# Remove HTML and URLs from the reviews\n",
        "data['review_body'] = data['review_body'].apply(lambda x: re.sub(r'(<.*?>|https?://\\S+)', '', x))\n",
        "\n",
        "# remove non-alphabetical characters\n",
        "data['review_body'] = data['review_body'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
        "\n",
        "# remove extra spaces\n",
        "data['review_body'] = data['review_body'].str.strip()\n",
        "\n",
        "# Perform contractions on the reviews\n",
        "data['review_body'] = data['review_body'].apply(lambda x: contractions.fix(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "acf1e8e6",
      "metadata": {
        "id": "acf1e8e6",
        "outputId": "6c6d7cc1-e1f7-443f-d419-192d18aeabf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average review length before and after cleaning: 268.995 , 265.76646666666664\n"
          ]
        }
      ],
      "source": [
        "# Print average length of reviews before and after cleaning\n",
        "review_lengths = data['review_body'].str.len()\n",
        "review_len_after_cleaning = review_lengths.mean()\n",
        "print(\"Average review length before and after cleaning:\", review_len_before_cleaning,\",\", review_len_after_cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4e6374d6",
      "metadata": {
        "id": "4e6374d6"
      },
      "outputs": [],
      "source": [
        "# remove stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "data['review_body'] = data['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_list]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "be454871",
      "metadata": {
        "id": "be454871"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "data['review_body'] = data['review_body'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8f78f8d2",
      "metadata": {
        "id": "8f78f8d2",
        "outputId": "bbf7e6cb-30d6-4efd-995d-ca1429556812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average review length before and after preprocessing: 265.76646666666664 , 155.29416666666665\n"
          ]
        }
      ],
      "source": [
        "# Print average length of reviews before and after preprocessing\n",
        "review_lengths = data['review_body'].str.len()\n",
        "review_len_after_preprocessing = review_lengths.mean()\n",
        "print(\"Average review length before and after preprocessing:\", review_len_after_cleaning, \",\", review_len_after_preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "fc01294b",
      "metadata": {
        "id": "fc01294b"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['review_body'], data['class'], stratify= data['class'], test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = data.sample(frac = 0.8, random_state=65) \n",
        "testing_dataset = data.drop(training_dataset.index) \n",
        "training_dataset = data.reset_index(drop=True) \n",
        "testing_dataset = data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "dz9ucNW0obWk"
      },
      "id": "dz9ucNW0obWk",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e51cf391",
      "metadata": {
        "id": "e51cf391"
      },
      "source": [
        "### Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff588190",
      "metadata": {
        "id": "ff588190"
      },
      "source": [
        "#### 2. a:   Load the pretrained “word2vec-google-news-300” Word2Vec model and check semantic similarities of the generated vectors using three examples of your own"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba3d0d2e",
      "metadata": {
        "id": "ba3d0d2e"
      },
      "outputs": [],
      "source": [
        "#loading the pretrained word2vec model\n",
        "#wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e2324541",
      "metadata": {
        "id": "e2324541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2661d86c-55d1-4e8d-c882-e07bc37de41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "#pretrained = api.load('word2vec-google-news-300')\n",
        "#pretrained.save('word2vec-google-news.kv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = gensim.models.KeyedVectors.load('word2vec-google-news.kv')"
      ],
      "metadata": {
        "id": "ENHz5t3mBe-r"
      },
      "id": "ENHz5t3mBe-r",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "14665e9e",
      "metadata": {
        "id": "14665e9e"
      },
      "source": [
        "> checking semantic similarities between vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = pretrained.most_similar(positive=['excellent','outstanding'], topn=1)\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Yo1f_R5SX6",
        "outputId": "dddab97e-55a1-452a-8129-203ab0428b9a"
      },
      "id": "E2Yo1f_R5SX6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('oustanding', 0.750198483467102)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "281729fc",
      "metadata": {
        "id": "281729fc",
        "outputId": "6a41569e-e895-4bb8-f231-f7a8c5e2c987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('puppy', 0.8089798092842102)]\n"
          ]
        }
      ],
      "source": [
        "similarity_1a = pretrained.most_similar(positive=['cat','dog'], topn=1)\n",
        "print(similarity_1a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9085018",
      "metadata": {
        "id": "d9085018",
        "outputId": "9471c311-373c-4464-db0a-927a65bc7da4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('glad', 0.7112970352172852)]\n"
          ]
        }
      ],
      "source": [
        "similarity_2a = pretrained.most_similar(positive=['happy','sad'], topn=1)\n",
        "print(similarity_2a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f323f3",
      "metadata": {
        "id": "66f323f3",
        "outputId": "c43c796f-fdbb-483a-b84d-343ea142f66c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('laptop_computer', 0.7891943454742432)]\n"
          ]
        }
      ],
      "source": [
        "similarity_3a = pretrained.most_similar(positive=['laptop','computer'], topn=1)\n",
        "print(similarity_3a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70008e6e",
      "metadata": {
        "id": "70008e6e",
        "outputId": "bfa37d7c-5e19-4674-8c1b-e57ebf41c06d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('laptops', 0.8053741455078125), ('laptop_computer', 0.7848465442657471), ('notebook', 0.67857825756073), ('netbook', 0.6707928776741028), ('computer', 0.6640493273735046), ('laptop_computers', 0.6633791327476501), ('notebook_PC', 0.6631842851638794), ('MacBook', 0.6598750352859497), ('PowerBook', 0.6520565748214722), ('Sony_Vaio_laptop', 0.6496157050132751)]\n"
          ]
        }
      ],
      "source": [
        "print(pretrained.most_similar('laptop'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c28f9684",
      "metadata": {
        "id": "c28f9684",
        "outputId": "3a9f8d76-c49f-47c1-8464-6f9b3e1c0da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('computers', 0.7979379892349243), ('laptop', 0.6640493273735046), ('laptop_computer', 0.6548868417739868), ('Computer', 0.647333562374115), ('com_puter', 0.6082080006599426), ('technician_Leonard_Luchko', 0.5662748217582703), ('mainframes_minicomputers', 0.5617720484733582), ('laptop_computers', 0.5585449934005737), ('PC', 0.5539618730545044), ('maker_Dell_DELL.O', 0.5519254207611084)]\n"
          ]
        }
      ],
      "source": [
        "print(pretrained.most_similar('computer'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. b: Train a Word2Vec model using your own dataset and heck the semantic similarities for the same two examples in part (a)"
      ],
      "metadata": {
        "id": "jdPxaNnQ8Tvi"
      },
      "id": "jdPxaNnQ8Tvi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7817aee",
      "metadata": {
        "id": "e7817aee"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences= data['review_body'].apply(lambda x: nltk.word_tokenize(x)), vector_size=300, window=13, min_count=9) \n",
        "#model.save(\"word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5237c8b",
      "metadata": {
        "id": "e5237c8b",
        "outputId": "dd1f578f-4944-4a8f-f48e-a53642e8c972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('gagging', 0.7637972831726074)]\n"
          ]
        }
      ],
      "source": [
        "similarity_1b = model.wv.most_similar(positive=['cat','dog'], topn=1)\n",
        "print(similarity_1b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18214234",
      "metadata": {
        "id": "18214234",
        "outputId": "dcc78d64-2bb6-453f-ad9c-a8c3cf86c14a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('glad', 0.6962702870368958)]\n"
          ]
        }
      ],
      "source": [
        "similarity_2b = model.wv.most_similar(positive=['happy','sad'], topn=1)\n",
        "print(similarity_2b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973bf222",
      "metadata": {
        "id": "973bf222",
        "outputId": "548281b6-db4a-4bad-b8c8-273054d19381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('booty', 0.8579108119010925)]\n"
          ]
        }
      ],
      "source": [
        "similarity_3b = model.wv.most_similar(positive=['laptop','computer'], topn=1)\n",
        "print(similarity_3b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efecd4dc",
      "metadata": {
        "id": "efecd4dc",
        "outputId": "3d38bd5d-1cec-415e-ff85-7c1b8e2eb424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('zippered', 0.8697407841682434), ('organized', 0.8600019216537476), ('handbag', 0.8476057052612305), ('cigarette', 0.8442824482917786), ('distorted', 0.8426803946495056), ('sanitary', 0.8353635668754578), ('pod', 0.8347823619842529), ('walker', 0.8314021229743958), ('divider', 0.8270478844642639), ('huh', 0.824307382106781)]\n"
          ]
        }
      ],
      "source": [
        "print(model.wv.most_similar('laptop'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066ad650",
      "metadata": {
        "id": "066ad650",
        "outputId": "988cc016-ca5d-4f74-a22a-8524c321ec72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('walked', 0.8297677636146545), ('conversation', 0.7957174181938171), ('prepaid', 0.7937031388282776), ('dumb', 0.7833716869354248), ('wtf', 0.7814996242523193), ('accident', 0.770950973033905), ('spirit', 0.770362377166748), ('sept', 0.7668968439102173), ('avoided', 0.7608656883239746), ('upsetting', 0.7600155472755432)]\n"
          ]
        }
      ],
      "source": [
        "print(model.wv.most_similar('computer'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f93116",
      "metadata": {
        "id": "e7f93116"
      },
      "source": [
        "#### What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d543ee9d",
      "metadata": {
        "id": "d543ee9d"
      },
      "source": [
        "> Based on comparison of the semantic similarities between vectors for the 2 models, the word2vec model created using amazon reviews data gives results with a higher similarity score. But, the results from pre-trained google news word2vec model are more logical, so overall we can't say that one model is better than the other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef10a925",
      "metadata": {
        "id": "ef10a925"
      },
      "source": [
        "### 3. Simple models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8af930f7",
      "metadata": {
        "id": "8af930f7"
      },
      "source": [
        "### Pre-trained word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use the pre-trained word2vec model to extract features from the tokenized text data by filtering out non-existent words, retrieving their word vectors, and calculating the average vector for each sentence"
      ],
      "metadata": {
        "id": "DUD-N9hLIsL3"
      },
      "id": "DUD-N9hLIsL3"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "ca7aeabd",
      "metadata": {
        "id": "ca7aeabd"
      },
      "outputs": [],
      "source": [
        "def get_word2vec_features(X, Y, word2vec_model):\n",
        "    wv_X = []\n",
        "    wv_Y = []\n",
        "    for sentence, label in zip(X, Y):\n",
        "        tokens = word_tokenize(sentence)\n",
        "        # Get only the tokens that exist in the word2vec model\n",
        "        filtered_tokens = [token for token in tokens if token in word2vec_model.key_to_index]\n",
        "        #filtered_tokens = [token for token in tokens if token in word2vec_model.vocab]\n",
        "        # If no tokens are left, skip this sentence\n",
        "        if not filtered_tokens:\n",
        "            continue\n",
        "        # Get the word vectors for the filtered tokens\n",
        "        vectors = [word2vec_model.get_vector(token) for token in filtered_tokens]\n",
        "        # Calculate the average vector for the sentence\n",
        "        average_vector = sum(vectors) / len(vectors)\n",
        "        wv_X.append(average_vector)\n",
        "        wv_Y.append(label)\n",
        "    return wv_X, wv_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "ebb975c2",
      "metadata": {
        "scrolled": true,
        "id": "ebb975c2"
      },
      "outputs": [],
      "source": [
        "X_train_wv, Y_train_wv = get_word2vec_features(X_train, y_train, pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "64e1e76e",
      "metadata": {
        "id": "64e1e76e"
      },
      "outputs": [],
      "source": [
        "X_test_wv, Y_test_wv = get_word2vec_features(X_test, y_test, pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "99d977dc",
      "metadata": {
        "id": "99d977dc",
        "outputId": "ea8aafe7-94e5-4b02-bbc2-c9e18752ef0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for perceptron model on pre-trained word2vec: 47.44842562432139\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train Perceptron and test data\n",
        "perceptron_wv = Perceptron(max_iter = 75, eta0 = 0.005, random_state=65)\n",
        "perceptron_wv.fit(X_train_wv, Y_train_wv) \n",
        "y_pred_perceptron_wv = perceptron_wv.predict(X_test_wv)\n",
        "\n",
        "# Compute the precision, recall, and f1-score per class\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(Y_test_wv, y_pred_perceptron_wv, average=None)\n",
        "\n",
        "# Compute the average precision, recall, and f1-score\n",
        "average_precision = precision.mean()\n",
        "average_recall = recall.mean()\n",
        "average_f1 = f1.mean()\n",
        "\n",
        "# Compute the accuracy\n",
        "acc = accuracy_score(Y_test_wv, y_pred_perceptron_wv)\n",
        "\n",
        "#print(\"Precision for perceptron model on pre-trained word2vec:\" ,average_precision*100) \n",
        "#print(\"Recall for perceptron model on pre-trained word2vec:\" ,average_recall*100) \n",
        "#print(\"F1 Score for perceptron model on pre-trained word2vec:\", average_f1*100)\n",
        "print(\"Test Accuracy for perceptron model on pre-trained word2vec:\", acc*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "e34988b3",
      "metadata": {
        "id": "e34988b3",
        "outputId": "5c15ee77-d6a6-41c9-ce87-584d83eb2e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for SVC model on pre-trained word2vec: 62.45719535621815\n"
          ]
        }
      ],
      "source": [
        "# Train SVC and test data\n",
        "svc_wv = LinearSVC(max_iter = 1000, random_state=65)\n",
        "svc_wv.fit(X_train_wv, Y_train_wv) \n",
        "y_pred_svc_wv = svc_wv.predict(X_test_wv)\n",
        "\n",
        "# Compute the precision, recall, and f1-score per class\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(Y_test_wv, y_pred_svc_wv, average=None)\n",
        "\n",
        "# Compute the average precision, recall, and f1-score\n",
        "average_precision = precision.mean()\n",
        "average_recall = recall.mean()\n",
        "average_f1 = f1.mean()\n",
        "\n",
        "# Compute the accuracy\n",
        "acc = accuracy_score(Y_test_wv, y_pred_svc_wv)\n",
        "\n",
        "#print(\"Precision for SVC model on pre-trained word2vec:\" ,average_precision*100) \n",
        "#print(\"Recall for SVC model on pre-trained word2vec:\" ,average_recall*100) \n",
        "#print(\"F1 Score for SVC model on pre-trained word2vec:\", average_f1*100)\n",
        "print(\"Test Accuracy for SVC model on pre-trained word2vec:\", acc*100) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74423867",
      "metadata": {
        "id": "74423867"
      },
      "source": [
        "### Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "b1b7c9c9",
      "metadata": {
        "id": "b1b7c9c9"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "X_train_tf = tfidf.fit_transform(X_train) \n",
        "X_test_tf = tfidf.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "414ced05",
      "metadata": {
        "id": "414ced05",
        "outputId": "2b7558e9-a8f2-43c1-d90f-1255587b2489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for perceptron model with tf-idf: 58.80833333333333\n"
          ]
        }
      ],
      "source": [
        "# Train Perceptron and test data\n",
        "perceptron_tf = Perceptron(max_iter = 75, eta0 = 0.005, random_state=65)\n",
        "perceptron_tf.fit(X_train_tf, y_train) \n",
        "y_pred_perceptron_tf = perceptron_tf.predict(X_test_tf)\n",
        "\n",
        "# Compute the precision, recall, and f1-score per class\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_perceptron_tf, average=None)\n",
        "\n",
        "# Compute the average precision, recall, and f1-score\n",
        "average_precision = precision.mean()\n",
        "average_recall = recall.mean()\n",
        "average_f1 = f1.mean()\n",
        "\n",
        "# Compute the accuracy\n",
        "acc = accuracy_score(y_test, y_pred_perceptron_tf)\n",
        "\n",
        "#print(\"Precision for perceptron model with tf-idf :\" ,average_precision*100) \n",
        "#print(\"Recall for perceptron model with tf-idf:\" ,average_recall*100) \n",
        "#print(\"F1 Score for perceptron model with tf-idf:\", average_f1*100)\n",
        "print(\"Test Accuracy for perceptron model with tf-idf:\", acc*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "ca89e44f",
      "metadata": {
        "id": "ca89e44f",
        "outputId": "a13603d1-88b9-4242-9836-dedbe401688f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for SVC model with tf-idf: 66.10833333333333\n"
          ]
        }
      ],
      "source": [
        "# Train SVC and test data\n",
        "svc_tf = LinearSVC(max_iter = 1000, random_state=65)\n",
        "svc_tf.fit(X_train_tf, y_train) \n",
        "y_pred_svc_tf = svc_tf.predict(X_test_tf)\n",
        "\n",
        "# Compute the precision, recall, and f1-score per class\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_svc_tf, average=None)\n",
        "\n",
        "# Compute the average precision, recall, and f1-score\n",
        "average_precision = precision.mean()\n",
        "average_recall = recall.mean()\n",
        "average_f1 = f1.mean()\n",
        "\n",
        "# Compute the accuracy\n",
        "acc = accuracy_score(y_test, y_pred_svc_tf)\n",
        "\n",
        "#print(\"Precision for SVC model with tf-idf :\" ,average_precision*100) \n",
        "#print(\"Recall for SVC model with tf-idf:\" ,average_recall*100) \n",
        "#print(\"F1 Score for SVC model with tf-idf:\", average_f1*100)\n",
        "print(\"Test Accuracy for SVC model with tf-idf:\", acc*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 3: Accuracy Summary<br>\n",
        "1. Using pre-trained word2vec: <br>\n",
        "  a. Perceptron accuracy = 47.448% <br>\n",
        "  b. SVM accuracy = 62.457% <br>\n",
        "\n",
        "2. Using tf-idf: <br>\n",
        "  a. Perceptron accuracy = 58.808% <br>\n",
        "  b. SVM accuracy = 66.108%"
      ],
      "metadata": {
        "id": "SbbKglCD2_xm"
      },
      "id": "SbbKglCD2_xm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What do you conclude from comparing performances for the models trained using the two different feature types"
      ],
      "metadata": {
        "id": "DXahZkHW8mrq"
      },
      "id": "DXahZkHW8mrq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Based on comparison of accuracy between the models using pre-trained features and the tf-idf features, tf-idf performs better. So we can say that tf-idf is a more robust input feature"
      ],
      "metadata": {
        "id": "5DXakGTE8uYW"
      },
      "id": "5DXakGTE8uYW"
    },
    {
      "cell_type": "markdown",
      "id": "b5abf2c5",
      "metadata": {
        "id": "b5abf2c5"
      },
      "source": [
        "### 4. FNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Using the Word2Vec features, train a feedforward multilayer perceptron network for classification"
      ],
      "metadata": {
        "id": "9f6aNFHO9ekS"
      },
      "id": "9f6aNFHO9ekS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are defining a multilayer perceptron (MLP) using PyTorch's nn.Sequential module. The MLP has three layers: an input layer with 300 nodes, a hidden layer with 100 nodes, and an output layer with 3 nodes.\n",
        "\n",
        "The activation function used in the hidden layers is ReLU (rectified linear unit)"
      ],
      "metadata": {
        "id": "vft4Di6cJZ4m"
      },
      "id": "vft4Di6cJZ4m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53496f7",
      "metadata": {
        "id": "e53496f7",
        "outputId": "659e3833-d13d-4f13-e89f-3fc0d5cf1d4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=10, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Feedforward MLP model network with two hidden layers, each with 100 and 10 nodes, respectively\n",
        "mlp = nn.Sequential(\n",
        "    # Input layer to hidden layer\n",
        "    nn.Linear(300, 100),\n",
        "    # ReLU activation function \n",
        "    nn. ReLU(), \n",
        "    # Hidden layer to output layer\n",
        "    nn.Linear(100, 10), \n",
        "    nn. ReLU(),\n",
        "    # Output layer \n",
        "    nn.Linear(10, 3))\n",
        "print(mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33924eb0",
      "metadata": {
        "id": "33924eb0"
      },
      "outputs": [],
      "source": [
        "# Loss function -> CrossEntropyLoss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Select Optimizer = Adam\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (a) Use the average Word2Vec vectors and train the neural network"
      ],
      "metadata": {
        "id": "yuUvp_QC9thC"
      },
      "id": "yuUvp_QC9thC"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "96b108d9",
      "metadata": {
        "id": "96b108d9"
      },
      "outputs": [],
      "source": [
        "#The train_model function is responsible for training a neural network model on a given dataset using the PyTorch library\n",
        "def train_model(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    mlp.train()\n",
        "    \n",
        "    # Iterate over the training data in mini-batches\n",
        "    for inputs, labels in train_loader:\n",
        "        # Reset the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass: compute the predicted outputs of the model\n",
        "        outputs = mlp(inputs)\n",
        "        # Compute the loss between the predicted outputs and the true labels\n",
        "        loss = loss_fn(outputs, labels) \n",
        "        # Backward pass: compute the gradients of the loss with respect to the model parameters\n",
        "        loss.backward()\n",
        "        # Update the model parameters using the computed gradients\n",
        "        optimizer.step() \n",
        "    # Print the current epoch number and the training loss every 10 epochs\n",
        "    #if epoch % 10 == 0:\n",
        "    print(f\"Epoch {epoch:4d} Loss: {loss.item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "603ea792",
      "metadata": {
        "id": "603ea792"
      },
      "outputs": [],
      "source": [
        "def test_model():\n",
        "    #set the MLP model to evaluation mode\n",
        "    mlp.eval()\n",
        "    #variable to count the number of correct predictions made by the model on the test dataset\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            #Feed the input data into the MLP model to get the predicted outputs\n",
        "            outputs = mlp(inputs)\n",
        "            #Find the predicted labels for each input sample\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            #Compare the predicted labels with the ground truth labels to count the number of correct predictions\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    #Calculate the overall accuracy on the test dataset \n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print('Accuracy on test set: {:.2f}%'.format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "0d46cccc",
      "metadata": {
        "id": "0d46cccc"
      },
      "outputs": [],
      "source": [
        "# Set data \n",
        "X_train = torch.Tensor(X_train_wv)\n",
        "X_test = torch.Tensor(X_test_wv)\n",
        "\n",
        "# changing classes to 0,1,2 by reducing each class number by 1\n",
        "# Subtract 1 from each element in Y_train_wv\n",
        "Y_train_wv = [y - 1 for y in Y_train_wv]\n",
        "# Subtract 1 from each element in Y_test_wv\n",
        "Y_test_wv = [y - 1 for y in Y_test_wv]\n",
        "\n",
        "y_train = torch.LongTensor(Y_train_wv) \n",
        "y_test = torch.LongTensor(Y_test_wv) \n",
        "\n",
        "train_data = TensorDataset(X_train, y_train) \n",
        "test_data = TensorDataset(X_test, y_test) \n",
        "\n",
        "#create data loaders to load batches of input features and output labels during training and testing\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True) \n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "14d3dd60",
      "metadata": {
        "id": "14d3dd60",
        "outputId": "827005e5-6e20-4e07-93a3-d26ae7b2024f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0 Loss: 0.348462\n",
            "Epoch    1 Loss: 0.146178\n",
            "Epoch    2 Loss: 0.649950\n",
            "Epoch    3 Loss: 0.144198\n",
            "Epoch    4 Loss: 0.480346\n",
            "Epoch    5 Loss: 0.320843\n",
            "Epoch    6 Loss: 1.253019\n",
            "Epoch    7 Loss: 0.980260\n",
            "Epoch    8 Loss: 0.313839\n",
            "Epoch    9 Loss: 0.476107\n",
            "Epoch   10 Loss: 0.355499\n",
            "Epoch   11 Loss: 0.423981\n",
            "Epoch   12 Loss: 0.272197\n",
            "Epoch   13 Loss: 0.043827\n",
            "Epoch   14 Loss: 0.152745\n",
            "Epoch   15 Loss: 0.045686\n",
            "Epoch   16 Loss: 0.205988\n",
            "Epoch   17 Loss: 0.394859\n",
            "Epoch   18 Loss: 0.252818\n",
            "Epoch   19 Loss: 0.467688\n",
            "Epoch   20 Loss: 0.374966\n",
            "Epoch   21 Loss: 0.418058\n",
            "Epoch   22 Loss: 0.320456\n",
            "Epoch   23 Loss: 0.198235\n",
            "Epoch   24 Loss: 0.643628\n",
            "Epoch   25 Loss: 0.893644\n",
            "Epoch   26 Loss: 0.238316\n",
            "Epoch   27 Loss: 0.031611\n",
            "Epoch   28 Loss: 0.337153\n",
            "Epoch   29 Loss: 0.891692\n",
            "Epoch   30 Loss: 0.232392\n",
            "Epoch   31 Loss: 0.075726\n",
            "Epoch   32 Loss: 0.066815\n",
            "Epoch   33 Loss: 0.084214\n",
            "Epoch   34 Loss: 0.188040\n",
            "Epoch   35 Loss: 0.313921\n",
            "Epoch   36 Loss: 0.155949\n",
            "Epoch   37 Loss: 0.217146\n",
            "Epoch   38 Loss: 0.613333\n",
            "Epoch   39 Loss: 0.037952\n",
            "Epoch   40 Loss: 0.200539\n",
            "Epoch   41 Loss: 0.249139\n",
            "Epoch   42 Loss: 0.008191\n",
            "Epoch   43 Loss: 0.024634\n",
            "Epoch   44 Loss: 0.496579\n",
            "Epoch   45 Loss: 0.189970\n",
            "Epoch   46 Loss: 0.085708\n",
            "Epoch   47 Loss: 0.005218\n",
            "Epoch   48 Loss: 0.270721\n",
            "Epoch   49 Loss: 0.084385\n",
            "Epoch   50 Loss: 0.235630\n",
            "Epoch   51 Loss: 0.236722\n",
            "Epoch   52 Loss: 0.206839\n",
            "Epoch   53 Loss: 0.867669\n",
            "Epoch   54 Loss: 0.398141\n",
            "Epoch   55 Loss: 0.084354\n",
            "Epoch   56 Loss: 0.073553\n",
            "Epoch   57 Loss: 0.561730\n",
            "Epoch   58 Loss: 0.029989\n",
            "Epoch   59 Loss: 0.177583\n",
            "Epoch   60 Loss: 0.054957\n",
            "Epoch   61 Loss: 0.125054\n",
            "Epoch   62 Loss: 0.231713\n",
            "Epoch   63 Loss: 0.087247\n",
            "Epoch   64 Loss: 0.609809\n",
            "Epoch   65 Loss: 0.152007\n",
            "Epoch   66 Loss: 0.096239\n",
            "Epoch   67 Loss: 0.123272\n",
            "Epoch   68 Loss: 0.087379\n",
            "Epoch   69 Loss: 0.048163\n",
            "Epoch   70 Loss: 0.025534\n",
            "Epoch   71 Loss: 0.358235\n",
            "Epoch   72 Loss: 0.399552\n",
            "Epoch   73 Loss: 0.430765\n",
            "Epoch   74 Loss: 0.065172\n",
            "Epoch   75 Loss: 0.268065\n",
            "Epoch   76 Loss: 0.030238\n",
            "Epoch   77 Loss: 0.056783\n",
            "Epoch   78 Loss: 0.434328\n",
            "Epoch   79 Loss: 0.654119\n",
            "Epoch   80 Loss: 0.057290\n",
            "Epoch   81 Loss: 0.106597\n",
            "Epoch   82 Loss: 0.046246\n",
            "Epoch   83 Loss: 0.435028\n",
            "Epoch   84 Loss: 0.638065\n",
            "Epoch   85 Loss: 0.123029\n",
            "Epoch   86 Loss: 0.010198\n",
            "Epoch   87 Loss: 0.107779\n",
            "Epoch   88 Loss: 0.049329\n",
            "Epoch   89 Loss: 0.269081\n",
            "Epoch   90 Loss: 0.035462\n",
            "Epoch   91 Loss: 0.241868\n",
            "Epoch   92 Loss: 0.088481\n",
            "Epoch   93 Loss: 0.001846\n",
            "Epoch   94 Loss: 0.044079\n",
            "Epoch   95 Loss: 0.019699\n",
            "Epoch   96 Loss: 0.169109\n",
            "Epoch   97 Loss: 0.041571\n",
            "Epoch   98 Loss: 0.100528\n",
            "Epoch   99 Loss: 0.224995\n",
            "Accuracy on test set: 56.94%\n"
          ]
        }
      ],
      "source": [
        "# Train with epoch = 100, and test data\n",
        "for epoch in range(100): \n",
        "    train_model(epoch)\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6dff200",
      "metadata": {
        "id": "b6dff200"
      },
      "source": [
        "#### (b) concatenate the first 10 Word2Vec vectors for each review as the input feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are concatenating the first 10 word2vec vectors using the concatenate_word2vec function. \n",
        "This function tokenizes the sentence using nltk.word_tokenize and filters out tokens that are not present in the word2vec_model vocabulary.\n",
        "It then retrieves the word embeddings for the remaining tokens using the word2vec_model and concatenates the first 10 word embeddings into a single vector and pads the concatenated vector with zeros to ensure that it has a fixed length of 3000."
      ],
      "metadata": {
        "id": "w7Ddv06NJ8z0"
      },
      "id": "w7Ddv06NJ8z0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b38c0d",
      "metadata": {
        "id": "07b38c0d"
      },
      "outputs": [],
      "source": [
        "def concatenate_word2vec(X, Y, word2vec_model):\n",
        "    wv_X_c = []\n",
        "    wv_Y_c = []\n",
        "    for sentence, label in zip(X, Y):\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        #filtered_tokens = [token for token in tokens if token in word2vec_model.key_to_index]\n",
        "        filtered_tokens = [token for token in tokens if token in word2vec_model.vocab]\n",
        "        if len(filtered_tokens) > 0:\n",
        "            embeddings = [word2vec_model[token] for token in filtered_tokens[:10]]\n",
        "            concatenated = np.concatenate(embeddings)\n",
        "            padded = np.pad(concatenated, (0, 3000 - len(concatenated)), 'constant', constant_values=0)\n",
        "            wv_X_c.append(padded)\n",
        "            wv_Y_c.append(label)\n",
        "    return wv_X_c, wv_Y_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "37034b35",
      "metadata": {
        "id": "37034b35"
      },
      "outputs": [],
      "source": [
        "temp_X, temp_Y = concatenate_word2vec(data['review_body'], data['class'], pretrained)\n",
        "X_train_wv_c, X_test_wv_c, Y_train_wv_c, Y_test_wv_c = train_test_split(temp_X, temp_Y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The train_model function is responsible for training a neural network model on a given dataset using the PyTorch library\n",
        "def train_model(epochs):\n",
        "    # Set the model to training mode\n",
        "    mlp.train()\n",
        "    \n",
        "    # Iterate over the training data in mini-batches\n",
        "    for inputs, labels in train_loader:\n",
        "        # Reset the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass: compute the predicted outputs of the model\n",
        "        outputs = mlp(inputs)\n",
        "        # Compute the loss between the predicted outputs and the true labels\n",
        "        loss = loss_fn(outputs, labels) \n",
        "        # Backward pass: compute the gradients of the loss with respect to the model parameters\n",
        "        loss.backward()\n",
        "        # Update the model parameters using the computed gradients\n",
        "        optimizer.step() \n",
        "    # Print the current epoch number and the training loss every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"Loss: {loss.item():.6f}\")"
      ],
      "metadata": {
        "id": "qqSdJfpMyMCe"
      },
      "id": "qqSdJfpMyMCe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "    #set the MLP model to evaluation mode\n",
        "    mlp.eval()\n",
        "    #variable to count the number of correct predictions made by the model on the test dataset\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            #Feed the input data into the MLP model to get the predicted outputs\n",
        "            outputs = mlp(inputs)\n",
        "            #Find the predicted labels for each input sample\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            #Compare the predicted labels with the ground truth labels to count the number of correct predictions\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    #Calculate the overall accuracy on the test dataset \n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print('Accuracy on test set: {:.0f}%'.format(accuracy))"
      ],
      "metadata": {
        "id": "9wPUlmooyQXj"
      },
      "id": "9wPUlmooyQXj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "ddf2d79f",
      "metadata": {
        "id": "ddf2d79f",
        "outputId": "68c0c891-3f31-4718-e26a-4660d32dbb7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=300, out_features=100, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=10, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Feedforward MLP model\n",
        "mlp = nn.Sequential(\n",
        "    # Input layer to hidden layer\n",
        "    nn.Linear(300, 100),\n",
        "    # ReLU activation function\n",
        "    nn. ReLU(), \n",
        "    # Hidden layer to output layer\n",
        "    nn.Linear(100, 10), \n",
        "    nn. ReLU(),\n",
        "    # Output layer \n",
        "    nn.Linear(10, 3))\n",
        "print(mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "b27beff9",
      "metadata": {
        "id": "b27beff9"
      },
      "outputs": [],
      "source": [
        "# Loss function -> CrossEntropyLoss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Select Optimizer = Adam\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "217625c0",
      "metadata": {
        "id": "217625c0"
      },
      "outputs": [],
      "source": [
        "# Set data \n",
        "X_train = torch.Tensor(X_train_wv_c)\n",
        "X_test = torch.Tensor(X_test_wv_c)\n",
        "\n",
        "# changing classes to 0,1,2 by reducing each class number by 1\n",
        "# Subtract 1 from each element in Y_train_wv\n",
        "Y_train_wv_c = [y - 1 for y in Y_train_wv_c]\n",
        "# Subtract 1 from each element in Y_test_wv\n",
        "Y_test_wv_c = [y - 1 for y in Y_test_wv_c]\n",
        "\n",
        "y_train = torch.LongTensor(Y_train_wv_c) \n",
        "y_test = torch.LongTensor(Y_test_wv_c) \n",
        "\n",
        "train_data = TensorDataset(X_train, y_train) \n",
        "test_data = TensorDataset(X_test, y_test) \n",
        "\n",
        "#create data loaders to load batches of input features and output labels during training and testing\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True) \n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with epoch = 100, and test data\n",
        "for epoch in range(50): \n",
        "    train_model(epoch)\n",
        "test_model()"
      ],
      "metadata": {
        "id": "UyU06f7-nuLn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "473ac15c-4c10-419d-b615-4550d06a1b8c"
      },
      "id": "UyU06f7-nuLn",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.577560\n",
            "Loss: 0.346125\n",
            "Loss: 0.493343\n",
            "Loss: 0.513139\n",
            "Loss: 0.253340\n",
            "Accuracy on test set: 60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 4: Accuracy Summary\n",
        "##### 4.a Accuracy = 56.94%\n",
        "##### 4.b Accuracy = 60%"
      ],
      "metadata": {
        "id": "wy2WVlEZ2jSa"
      },
      "id": "wy2WVlEZ2jSa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section?"
      ],
      "metadata": {
        "id": "h7InOBXa_Toy"
      },
      "id": "h7InOBXa_Toy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> According to the accuracy scores for the simple models and the Feedforward Neural Networks, SVM using tf-idf features has the best performance"
      ],
      "metadata": {
        "id": "sRRkoZxZ_asV"
      },
      "id": "sRRkoZxZ_asV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. RNN"
      ],
      "metadata": {
        "id": "zVUJE-qhpGXl"
      },
      "id": "zVUJE-qhpGXl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using the Word2Vec features, train a recurrent neural network (RNN) for classification"
      ],
      "metadata": {
        "id": "jiQjD4cZ-F0w"
      },
      "id": "jiQjD4cZ-F0w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pad_reviews function pads or truncates a list of reviews to a specified maximum length. If a review is longer than the max_length, the function truncates it to max_length by slicing the first max_length elements of the review. If a review is shorter than max_length, the function pads it with zeros by concatenating the review with a list of max_length - len(review) zeros."
      ],
      "metadata": {
        "id": "u_YpezkkFhqw"
      },
      "id": "u_YpezkkFhqw"
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_reviews(reviews, max_length):\n",
        "    padded_reviews = []\n",
        "    for review in reviews:\n",
        "        if len(review) > max_length:\n",
        "            # Truncate longer reviews\n",
        "            padded_review = review[:max_length]\n",
        "        else:\n",
        "            # Pad shorter reviews with zeros\n",
        "            padded_review = review + [0] * (max_length - len(review))\n",
        "        padded_reviews.append(padded_review)\n",
        "    return padded_reviews"
      ],
      "metadata": {
        "id": "GrfQzk3qshyp"
      },
      "id": "GrfQzk3qshyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I have writeen a function that converts a list of tokenized reviews into a list of integer reviews. For each review, the function creates a new list called int_review. It then iterates over each word in the input review and checks whether that word is in the pre-trained model. If the word is in the model, the function retrieves the index number of that word in the model using the key_to_index attribute of the pretrained model. If the word is not in the model, the function assigns the index number 0 to that word."
      ],
      "metadata": {
        "id": "ZbTojRvzFxyV"
      },
      "id": "ZbTojRvzFxyV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the tokenized reviews to int type\n",
        "def convert_reviews_to_int(reviews):\n",
        "  int_reviews = []\n",
        "  for review in reviews:\n",
        "  # if specific word is in my word2vec model -> use index number. If not, put 0 instead of the words' index.\n",
        "    int_reviews.append([pretrained.key_to_index[word] if word in pretrained.key_to_index else 0 for word in review])\n",
        "  return int_reviews"
      ],
      "metadata": {
        "id": "QI2Jd_5ypK3o"
      },
      "id": "QI2Jd_5ypK3o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (a) Train a simple RNN for sentiment analysis"
      ],
      "metadata": {
        "id": "KRrwRQOOA2YA"
      },
      "id": "KRrwRQOOA2YA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Here, we are training a simple RNN for sentiment analysis using PyTorch. \n",
        "The RNN implementation has the following layers: <br>\n",
        "\n",
        "1. An embedding layer, which maps each input index to a dense vector of embedding_dim dimensions.<br>\n",
        "2. A layer with hidden_size hidden units. The batch_first=True argument specifies that the input tensor has dimensions (batch_size, sequence_length, embedding_dim). <br>\n",
        "3. A linear layer (fully connected layer) that maps the output of the previous layer to the output classes. <br>\n",
        "We use the CrossEntropyLoss loss function and the Adam optimizer with a learning rate of 0.001."
      ],
      "metadata": {
        "id": "9UWH-CgnDmpE"
      },
      "id": "9UWH-CgnDmpE"
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_size, num_classes):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(input_dim, hidden_size)\n",
        "    self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True, nonlinearity='relu') \n",
        "    self.fc = nn.Linear(hidden_size, num_classes)\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x) \n",
        "    out, _ = self.rnn(embedded) \n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "wRE6vS0CvLm8"
      },
      "id": "wRE6vS0CvLm8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, batch_size): \n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  # Train model with mini batch\n",
        "  for inputs, labels in train_loader:\n",
        "        # Reset the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass: compute the predicted outputs of the model\n",
        "        outputs = model(inputs)\n",
        "        # Compute the loss between the predicted outputs and the true labels\n",
        "        loss = loss_fn(outputs, labels.reshape(1,batch_size).t()) \n",
        "        # Backward pass: compute the gradients of the loss with respect to the model parameters\n",
        "        loss.backward()\n",
        "        # Update the model parameters using the computed gradients\n",
        "        optimizer.step() \n",
        "        epoch_loss += loss.item()\n",
        "  print('Loss: {:.6f}'.format(loss.item()))"
      ],
      "metadata": {
        "id": "zITzLnUBvgR4"
      },
      "id": "zITzLnUBvgR4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, data_loader):\n",
        "  #set the model to evaluation mode\n",
        "  model.eval()\n",
        "  #variable to count the number of correct predictions made by the model on the test dataset\n",
        "  correct = 0 \n",
        "\t#Create minibatch\n",
        "  with torch.no_grad():\n",
        "    for data, labels in data_loader:\n",
        "      #Feed the input data into the model to get the predicted outputs\n",
        "      outputs = model(data)\n",
        "\t\t\t#Find the predicted labels for each input sample\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      #Compare the predicted labels with the ground truth labels to count the number of correct predictions\n",
        "      correct += predicted.eq(labels.data.view_as(predicted)).sum()\n",
        "    #Print accuracy\n",
        "    data_num = len(data_loader.dataset)\n",
        "    #print('\\nAccuracy with test data: {}/{} ({:.0f}%)\\n'.format(correct,data_num, 100. * correct / data_num))\n",
        "    print('\\nAccuracy with test data: {:.2f}%\\n'.format(100. * correct / data_num))"
      ],
      "metadata": {
        "id": "c-WNsKXhhCZV"
      },
      "id": "c-WNsKXhhCZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change words in train data to number values using google-word2vec-news model\n",
        "x_train = convert_reviews_to_int(word_tokenize(sentence) for sentence in X_train)\n",
        "#padding shorter reviews with a null value (0) \n",
        "x_train = np.array(pad_reviews(x_train, 20))\n",
        "\n",
        "#Change words in test data to number values using google-word2vec-news model\n",
        "x_test = convert_reviews_to_int(word_tokenize(sentence) for sentence in X_test) \n",
        "#padding shorter reviews with a null value (0)\n",
        "x_test = np.array(pad_reviews(x_test, 20))"
      ],
      "metadata": {
        "id": "pDKiGEzC0VG2"
      },
      "id": "pDKiGEzC0VG2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "X_train = torch.LongTensor(x_train)\n",
        "X_test = torch.LongTensor(x_test)\n",
        "\n",
        "# changing classes to 0,1,2 by reducing each class number by 1\n",
        "y_train = [y - 1 for y in y_train]\n",
        "y_test = [y - 1 for y in y_test]\n",
        "Y_train = torch.LongTensor(y_train) \n",
        "Y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# Make a dataset and dataloader\n",
        "train_data = TensorDataset(X_train, Y_train) \n",
        "test_data = TensorDataset(X_test, Y_test) \n",
        "\n",
        "#create data loaders to load batches of input features and output labels during training and testing\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True) \n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "I1HaLw-x1V5h"
      },
      "id": "I1HaLw-x1V5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = len(pretrained)+1 \n",
        "hidden_dim = 20\n",
        "output_dim = 1\n",
        "model = RNN(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Loss function -> CrossEntropyLoss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Select Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "xvS8hFrV2GqN"
      },
      "id": "xvS8hFrV2GqN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5): \n",
        "  train(epoch, 64)\n",
        "test(model, test_loader)"
      ],
      "metadata": {
        "id": "cyVBlz0k2UcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c2d396-15a4-4910-a260-600429abd880"
      },
      "id": "cyVBlz0k2UcO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.989170\n",
            "Loss: 0.963754\n",
            "Loss: 0.982743\n",
            "Loss: 0.914864\n",
            "Loss: 0.858872\n",
            "\n",
            "Accuracy with test data: 52.08%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (b) Repeat part (a) by considering a gated recurrent unit cell"
      ],
      "metadata": {
        "id": "j9smId-dA8u4"
      },
      "id": "j9smId-dA8u4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Here, we are implementing a GRU (Gated Recurrent Unit) neural network using PyTorch. It uses an embedding layer followed by a single GRU layer with 20 hidden units, and a linear layer to map the output to the classes. We use the CrossEntropyLoss loss function and the Adam optimizer with a learning rate of 0.001."
      ],
      "metadata": {
        "id": "sNMlBtGAE0iJ"
      },
      "id": "sNMlBtGAE0iJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GRU model\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size, num_classes):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True) \n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x) \n",
        "        out, _ = self.gru(embedded) \n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "nEAqpKwCjUCR"
      },
      "id": "nEAqpKwCjUCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = len(pretrained)+1 \n",
        "hidden_dim = 20\n",
        "output_dim = 1\n",
        "model = GRU(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Loss function used: CrossEntropyLoss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Adam Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "oinXcHajjYPu"
      },
      "id": "oinXcHajjYPu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5): \n",
        "    train(epoch, 64)\n",
        "test(model,test_loader)"
      ],
      "metadata": {
        "id": "75AMkL95jbyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b2523f-48cc-4c1b-e0da-2339a3993f1c"
      },
      "id": "75AMkL95jbyR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.122906\n",
            "Loss: 1.068928\n",
            "Loss: 0.985442\n",
            "Loss: 1.010378\n",
            "Loss: 0.797255\n",
            "\n",
            "Accuracy with test data: 51.67%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (c) Repeat part (a) by considering an LSTM unit cell"
      ],
      "metadata": {
        "id": "JTsNlwN7BFBQ"
      },
      "id": "JTsNlwN7BFBQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The LSTM model uses an embedding layer followed by a single LSTM layer with 20 hidden units, and a linear layer to map the output to the classes. We use the CrossEntropyLoss loss function and the Adam optimizer with a learning rate of 0.001."
      ],
      "metadata": {
        "id": "lpKwWlZYDJn6"
      },
      "id": "lpKwWlZYDJn6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        out, _ = self.lstm(embedded)\n",
        "        #out = self.fc(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "5g0Ml2Cw3NNe"
      },
      "id": "5g0Ml2Cw3NNe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = len(pretrained)+1 \n",
        "hidden_dim = 20\n",
        "output_dim = 1\n",
        "model = LSTM(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Loss function used: CrossEntropyLoss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# Adam Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "mJz4THuG39Ly"
      },
      "id": "mJz4THuG39Ly",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5): \n",
        "    train(epoch, 64)\n",
        "test(model, test_loader)"
      ],
      "metadata": {
        "id": "im2-5RUQ4DFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9ad63a-0339-44b1-b5f6-f425151f599b"
      },
      "id": "im2-5RUQ4DFc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.151444\n",
            "Loss: 1.025713\n",
            "Loss: 1.108427\n",
            "Loss: 0.950206\n",
            "Loss: 0.910374\n",
            "\n",
            "Accuracy with test data: 52.12%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5: Accuracy Summary <br>\n",
        "5a: RNN accuracy: 52.12% <br>\n",
        "5b. GRU accuracy: 51.67% <br>\n",
        "5c: LSTM accuracy: 52.08%"
      ],
      "metadata": {
        "id": "ZSx2InXL38Zf"
      },
      "id": "ZSx2InXL38Zf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN"
      ],
      "metadata": {
        "id": "5ZmG-vqbATNG"
      },
      "id": "5ZmG-vqbATNG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Based on the accuracy values, LSTM performs the best but the simple RNN also comes close with very little difference in performance"
      ],
      "metadata": {
        "id": "0JspKZHjAdwD"
      },
      "id": "0JspKZHjAdwD"
    },
    {
      "cell_type": "code",
      "source": [
        "#References\n",
        "\n",
        "#https://radimrehurek.com/gensim/models/word2vec.html\n",
        "#https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
        "#https://arxiv.org/abs/1301.3781\n",
        "#https://www.kaggle.com/c/word2vec-nlp-tutorial/discussion/27022\n",
        "#https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf\n",
        "#https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
        "#https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "#https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
      ],
      "metadata": {
        "id": "gplGzmttLfyw"
      },
      "id": "gplGzmttLfyw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
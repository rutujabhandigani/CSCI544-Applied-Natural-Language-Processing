{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh3VqXg3375B"
      },
      "outputs": [],
      "source": [
        "#Importing the required libraries \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import random\n",
        "import json\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_train(dataset):\n",
        "    train_x = list()\n",
        "    train_y = list()\n",
        "    x = list() \n",
        "    y = list()\n",
        "    first = 1\n",
        "    for row in dataset.itertuples():\n",
        "        if(row.id == '1' and first == 0):\n",
        "            train_x.append(x)\n",
        "            train_y.append(y)\n",
        "            x = list()\n",
        "            y = list()\n",
        "        first = 0\n",
        "        x.append(row.word)\n",
        "        y.append(row.NER)\n",
        "\n",
        "    train_x.append(x)\n",
        "    train_y.append(y)\n",
        "\n",
        "    return train_x, train_y\n",
        "\n",
        "\n",
        "def read_file(path):\n",
        "    train_df = list()\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            if len(line) > 2:\n",
        "                id, word, ner_tag = line.strip().split(\" \")\n",
        "                train_df.append([id, word, ner_tag])\n",
        "\n",
        "    train_df = pd.DataFrame(train_df, columns=['id', 'word', 'NER'])\n",
        "    train_df = train_df.dropna()\n",
        "    train_x, train_y = prep_train(train_df)\n",
        "    return train_x, train_y"
      ],
      "metadata": {
        "id": "hkmYaTiElLWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_test(dataset):\n",
        "    train_x = list()\n",
        "    x = list()\n",
        "    first = 1\n",
        "    for row in dataset.itertuples():\n",
        "        if(row.id == '1' and first == 0):\n",
        "            train_x.append(x)\n",
        "            x = list()\n",
        "        first = 0\n",
        "        x.append(row.word)\n",
        "\n",
        "    train_x.append(x)\n",
        "    return train_x\n",
        "\n",
        "\n",
        "def read_test(path):\n",
        "    train_df = list()\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            if len(line) > 1:\n",
        "                id, word = line.strip().split(\" \")\n",
        "                train_df.append([id, word])\n",
        "\n",
        "    train_df = pd.DataFrame(train_df, columns=['id', 'word'])\n",
        "    train_df = train_df.dropna()\n",
        "    train_x = prep_test(train_df)\n",
        "    return train_x"
      ],
      "metadata": {
        "id": "3U0KjjnGlYuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbjSaM4E375C"
      },
      "outputs": [],
      "source": [
        "train_x, train_y = read_file('./data/train')\n",
        "val_x, val_y = read_file('./data/dev')\n",
        "test_x = read_test('./data/test')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BiLSTM: <br>\n",
        "The model takes as input a sequence of tokens represented as integers, and their corresponding lengths. It then passes the sequence through an embedding layer to obtain a dense representation for each token. The embedded sequence is then passed through a BiLSTM layer to capture the contextual information of each token. The output of the BiLSTM is passed through a linear layer followed by a non-linear activation function and another linear layer to produce the final output, which is a probability distribution over the set of tags."
      ],
      "metadata": {
        "id": "q492z0kPs1I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, hidden_dim, lstm_layers,\n",
        "                 bidirectional, dropout_val, num_classes):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        #Hyper Parameters\n",
        "        #hidden_dim = 256\n",
        "        self.hidden_dim = hidden_dim \n",
        "        #LSTM Layers = 1\n",
        "        self.lstm_layers = lstm_layers\n",
        "        #Embedding Dimension = 100\n",
        "        self.embedding_dim = embedding_dim\n",
        "        #Linear Ouput Dimension = 128\n",
        "        self.output_dim = output_dim\n",
        "        #the number of possible tags in the output\n",
        "        self.num_classes = num_classes\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        #Creating Network\n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.uniform_(-1,1)\n",
        "        self.LSTM = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers=lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*self.num_directions,\n",
        "                            output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_val)\n",
        "        self.elu = nn.ELU(alpha=0.01)\n",
        "        self.classifier = nn.Linear(output_dim, self.num_classes)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
        "                            batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.lstm_layers * self.num_directions,\n",
        "                            batch_size, self.hidden_dim).to(device))\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, input_seq, seq_len):\n",
        "        # Set initial states\n",
        "        batch_size = input_seq.shape[0]\n",
        "        h_0, c_0 = self.init_hidden(batch_size)\n",
        "        #Forward pass LSTM\n",
        "        #Embedding Layer\n",
        "        embedded = self.embedding(input_seq).float()\n",
        "        #LSTM layer\n",
        "        packed_embedded = pack_padded_sequence(\n",
        "            embedded, seq_len, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
        "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        #Output Layers\n",
        "        dropout = self.dropout(output_unpacked)\n",
        "        lin = self.fc(dropout)\n",
        "        pred = self.elu(lin)\n",
        "        pred = self.classifier(pred)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "TLB0x-aHnpR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BiLSTM Glove: <br>\n",
        "The network consists of an embedding layer that converts the integer representations of the input tokens to dense vectors, a bidirectional LSTM layer that processes the embedded sequence, a fully connected layer that applies a linear transformation to the output of the LSTM layer, a dropout layer that randomly sets some of the activations to zero during training to prevent overfitting, an ELU (Exponential Linear Unit) activation layer that applies the element-wise function f(x) = alpha * (exp(x) - 1) for x < 0 and f(x) = x for x >= 0, and a final linear classifier layer that maps the output to the number of classes in the classification task. <br>\n",
        "The LSTM layer is initialized with the specified number of layers, hidden size, and bidirectional flag. The input sequences are expected to be of variable length and are handled using PyTorch's pack_padded_sequence and pad_packed_sequence functions."
      ],
      "metadata": {
        "id": "6USV23vr3-rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM_Glove(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, hidden_dim, lstm_layers,\n",
        "                 bidirectional, dropout_val, num_classes, emb_matrix):\n",
        "        super(BiLSTM_Glove, self).__init__()\n",
        "        #Hyper Parameters\n",
        "        #hidden_dim = 256\n",
        "        self.hidden_dim = hidden_dim\n",
        "        #LSTM Layers = 1\n",
        "        self.lstm_layers = lstm_layers\n",
        "        #Embedding Dimension = 100\n",
        "        self.embedding_dim = embedding_dim\n",
        "        #Linear Ouput Dimension = 128\n",
        "        self.output_dim = output_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.emb_matrix = emb_matrix\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        #Creating Network\n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))\n",
        "        self.LSTM = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim,\n",
        "                            num_layers=lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*self.num_directions, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_val)\n",
        "        self.elu = nn.ELU(alpha=0.01)\n",
        "        self.classifier = nn.Linear(output_dim, self.num_classes)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
        "                            batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.lstm_layers * self.num_directions,\n",
        "                            batch_size, self.hidden_dim).to(device))\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, input_seq, seq_len):\n",
        "        #Set initial states\n",
        "        batch_size = input_seq.shape[0]\n",
        "        h_0, c_0 = self.init_hidden(batch_size)\n",
        "\n",
        "        #Forward pass LSTM\n",
        "        #Embedding Layer\n",
        "        embedded = self.embedding(input_seq).float()\n",
        "        #LSTM Layer\n",
        "        packed_embedded = pack_padded_sequence(embedded, seq_len, batch_first=True, enforce_sorted=False)\n",
        "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
        "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        #Output Layers\n",
        "        dropout = self.dropout(output_unpacked)\n",
        "        lin = self.fc(dropout)\n",
        "        pred = self.elu(lin)\n",
        "        pred = self.classifier(pred)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "Ha8QUiNunt8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BiLSTM_DataLoader <br>\n",
        "Here, inputs is a list of tokenized text data and targets is a list of labels corresponding to each input text instance. <br>\n",
        "__get_item__ method returns a tuple of two tensors where input_instance is a tensor of token indices representing one input text instance and target_instance is a tensor of the corresponding label. <br>\n",
        "### BiLSTM_TestLoader <br>\n",
        "Here, BiLSTM_TestLoader takes only one argument=> inputs which is a list of tokenized text data. <br>\n",
        "__getitem__ method returns a single tensor input_instance of token indices representing one input text instance. The purpose of this loader is to only load the test data during inference, where labels are not available."
      ],
      "metadata": {
        "id": "ow1kQvcv5Ulo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM_DataLoader(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_instance = torch.tensor(self.inputs[index])\n",
        "        target_instance = torch.tensor(self.targets[index])\n",
        "        return input_instance, target_instance\n",
        "\n",
        "class BiLSTM_TestLoader(Dataset):\n",
        "    def __init__(self, inputs):\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_instance = torch.tensor(self.inputs[index])\n",
        "        return input_instance\n"
      ],
      "metadata": {
        "id": "FBezuZq3oGjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class data_collator(object):\n",
        "\n",
        "    def __init__(self, vocab, label):\n",
        "        self.params = vocab\n",
        "        self.label = label\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        (input_data, target_data) = zip(*batch)\n",
        "        input_len = [len(x) for x in input_data]\n",
        "        target_len = [len(y) for y in target_data]\n",
        "        batch_max_len = max([len(s) for s in input_data])\n",
        "        batch_data = self.params['<PAD>']*np.ones((len(input_data), batch_max_len))\n",
        "        batch_labels = -1*np.zeros((len(input_data), batch_max_len))\n",
        "        for j in range(len(input_data)):\n",
        "            cur_len = len(input_data[j])\n",
        "            batch_data[j][:cur_len] = input_data[j]\n",
        "            batch_labels[j][:cur_len] = target_data[j]\n",
        "\n",
        "        batch_data, batch_labels = torch.LongTensor(\n",
        "            batch_data), torch.LongTensor(batch_labels)\n",
        "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
        "\n",
        "        return batch_data, batch_labels, input_len, target_len\n",
        "\n",
        "class test_data_collator(object):\n",
        "\n",
        "    def __init__(self, vocab, label):\n",
        "        self.params = vocab\n",
        "        self.label = label\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        input_data = batch\n",
        "        input_len = [len(x) for x in input_data]\n",
        "        batch_max_len = max([len(s) for s in input_data])\n",
        "        batch_data = self.params['<PAD>']*np.ones((len(input_data), batch_max_len))\n",
        "        for j in range(len(input_data)):\n",
        "            cur_len = len(input_data[j])\n",
        "            batch_data[j][:cur_len] = input_data[j]\n",
        "\n",
        "        batch_data = torch.LongTensor(batch_data)\n",
        "        batch_data = Variable(batch_data)\n",
        "\n",
        "        return batch_data, input_len"
      ],
      "metadata": {
        "id": "FLtdfQNUoPA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prepare_vocabulary function takes a dataset (list of sentences) as input and returns the set of unique words in the dataset."
      ],
      "metadata": {
        "id": "UC2H7eJz9UyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_vocabulary(dataset):\n",
        "\n",
        "    vocab = set()\n",
        "    for sentence in dataset:\n",
        "        for word in sentence:\n",
        "            vocab.add(word)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "F7KWUtqA7tS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare_word_index takes three lists of sentences (training, validation, and test) and creates a word-to-index dictionary that maps each unique word to a unique index."
      ],
      "metadata": {
        "id": "CG90Ep1i9cO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_word_index(train_data, val_data, test_data):\n",
        "\n",
        "    word_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    idx = 2\n",
        "\n",
        "    for data in [train_data, val_data, test_data]:\n",
        "        for sent in data:\n",
        "            for word in sent:\n",
        "                if word not in word_index:\n",
        "                    word_index[word] = idx\n",
        "                    idx += 1\n",
        "    #with open('vocab.json', 'w') as f:\n",
        "        #json.dump(word_index, f)\n",
        "    return word_index\n"
      ],
      "metadata": {
        "id": "cXqUwSHK7tKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vectorize_sentences function takes the training dataset and the word-to-index dictionary word_index as input and converts each sentence to a list of word indices based on the mapping in word_index."
      ],
      "metadata": {
        "id": "-vl0OQas9hmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sentences(train_data, word_index):\n",
        "\n",
        "    train_data_vec = list()\n",
        "    tmp_x = list()\n",
        "    for words in train_data:\n",
        "        for word in words:\n",
        "            tmp_x.append(word_index[word])\n",
        "        train_data_vec.append(tmp_x)\n",
        "        tmp_x = list()\n",
        "\n",
        "    return train_data_vec\n"
      ],
      "metadata": {
        "id": "qSfZoWYP7qvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare_label_dict takes the training and validation target datasets as input and returns a dictionary that maps each unique tag in the datasets to a unique index."
      ],
      "metadata": {
        "id": "_-JU0qK69vE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_label_dict(train_labels, val_labels):\n",
        "\n",
        "    label1 = prepare_vocabulary(train_labels)\n",
        "    label2 = prepare_vocabulary(val_labels)\n",
        "    labels = label1.union(label2)\n",
        "    label_tuples = []\n",
        "    counter = 0\n",
        "    for tag in labels:\n",
        "        label_tuples.append((tag, counter))\n",
        "        counter += 1\n",
        "    label_dict = dict(label_tuples)\n",
        "    #with open('label.json', 'w') as f:\n",
        "        #json.dump(label_dict, f)\n",
        "    return label_dict"
      ],
      "metadata": {
        "id": "7dj_KmEQ7qjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectorize_labels takes the training target dataset and the label dictionary as input and converts each target sequence to a list of tag indices based on the mapping in label_dict."
      ],
      "metadata": {
        "id": "05jCrI1G92vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_labels(train_labels, label_dict):\n",
        "\n",
        "    train_labels_vec = list()\n",
        "    for tags in train_labels:\n",
        "        tmp_tags = list()\n",
        "        for label in tags:\n",
        "            tmp_tags.append(label_dict[label])\n",
        "        train_labels_vec.append(tmp_tags)\n",
        "    return train_labels_vec"
      ],
      "metadata": {
        "id": "xgHAnrEu7qMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSvuBTPX375F"
      },
      "outputs": [],
      "source": [
        "word_idx = prepare_word_index(train_x, val_x, test_x)\n",
        "train_x_vec = vectorize_sentences(train_x, word_idx)\n",
        "test_x_vec = vectorize_sentences(test_x, word_idx)\n",
        "val_x_vec = vectorize_sentences(val_x, word_idx)\n",
        "label_dict = prepare_label_dict(train_y, val_y)\n",
        "train_y_vec = vectorize_labels(train_y, label_dict)\n",
        "val_y_vec = vectorize_labels(val_y, label_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The create_embedding_matrix function takes a word-to-index dictionary, a dictionary mapping words to their corresponding GloVe embeddings, and an embedding dimension as input. It returns a matrix where each row corresponds to a word in word_index and the corresponding row contains its GloVe embedding. If a word is not present in embedding_dict, its embedding is set to either the embedding of its lowercase version (if present in embedding_dict) or to an embedding."
      ],
      "metadata": {
        "id": "y7FcuPCyFVre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(word_index, embedding_dict, dimension):\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index), dimension))\n",
        "    for word, index in word_index.items():\n",
        "        if word in embedding_dict:\n",
        "            embedding_matrix[index] = embedding_dict[word]\n",
        "        else:\n",
        "            if word.lower() in embedding_dict:\n",
        "                embedding_matrix[index] = embedding_dict[word.lower()] + 5e-3\n",
        "            else:\n",
        "                embedding_matrix[index] = embedding_dict[\"<UNK>\"]\n",
        "    #np.save('embedding_matrix.npy', embedding_matrix)\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "iRNF7g3DFQZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are assigning class weights to handle class imbalance in the training data. By assigning higher weights to under-represented classes, the model is encouraged to pay more attention to these classes during training. The class weights are calculated using the formula:\n",
        "\n",
        "class_weight = ln(0.35 * total_number_of_tags / frequency_of_label)"
      ],
      "metadata": {
        "id": "TXTZSyCfURIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGp2lWY2375F"
      },
      "outputs": [],
      "source": [
        "def initialize_class_weights(label_dict, train_y, val_y):\n",
        "    class_weights = dict()\n",
        "    for key in label_dict:\n",
        "        class_weights[key] = 0\n",
        "    total_nm_tags = 0\n",
        "    for data in [train_y, val_y]:\n",
        "        for tags in data:\n",
        "            for tag in tags:\n",
        "                total_nm_tags += 1\n",
        "                class_weights[tag] += 1\n",
        "\n",
        "    class_wt = list()\n",
        "    for key in class_weights.keys():\n",
        "        if class_weights[key]:\n",
        "            score = round(math.log(0.35*total_nm_tags / class_weights[key]), 2)\n",
        "            class_weights[key] = score if score > 1.0 else 1.0\n",
        "        else:\n",
        "            class_weights[key] = 1.0\n",
        "        class_wt.append(class_weights[key])\n",
        "    class_wt = torch.tensor(class_wt)\n",
        "    return class_wt\n",
        "\n",
        "\n",
        "class_wt = initialize_class_weights(label_dict, train_y, val_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GmM5heb375F",
        "outputId": "665ecac2-ae4f-41a1-f45d-957106703703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiLSTM(\n",
            "  (embedding): Embedding(30292, 100)\n",
            "  (LSTM): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (dropout): Dropout(p=0.33, inplace=False)\n",
            "  (elu): ELU(alpha=0.01)\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "Epoch: 1 \tTraining Loss: 2.669663\n",
            "Epoch: 2 \tTraining Loss: 1.857107\n",
            "Epoch: 3 \tTraining Loss: 1.366157\n",
            "Epoch: 4 \tTraining Loss: 1.046358\n",
            "Epoch: 5 \tTraining Loss: 0.784568\n",
            "Epoch: 6 \tTraining Loss: 0.601561\n",
            "Epoch: 7 \tTraining Loss: 0.470704\n",
            "Epoch: 8 \tTraining Loss: 0.370218\n",
            "Epoch: 9 \tTraining Loss: 0.288217\n",
            "Epoch: 10 \tTraining Loss: 0.231052\n",
            "Epoch: 11 \tTraining Loss: 0.205535\n",
            "Epoch: 12 \tTraining Loss: 0.187001\n",
            "Epoch: 13 \tTraining Loss: 0.154262\n",
            "Epoch: 14 \tTraining Loss: 0.138245\n",
            "Epoch: 15 \tTraining Loss: 0.107758\n",
            "Epoch: 16 \tTraining Loss: 0.104288\n",
            "Epoch: 17 \tTraining Loss: 0.087305\n",
            "Epoch: 18 \tTraining Loss: 0.081432\n",
            "Epoch: 19 \tTraining Loss: 0.061895\n",
            "Epoch: 20 \tTraining Loss: 0.058798\n",
            "Epoch: 21 \tTraining Loss: 0.050315\n",
            "Epoch: 22 \tTraining Loss: 0.041090\n",
            "Epoch: 23 \tTraining Loss: 0.039136\n",
            "Epoch: 24 \tTraining Loss: 0.034539\n",
            "Epoch: 25 \tTraining Loss: 0.033696\n",
            "Epoch: 26 \tTraining Loss: 0.033203\n",
            "Epoch: 27 \tTraining Loss: 0.036485\n",
            "Epoch: 28 \tTraining Loss: 0.034371\n",
            "Epoch: 29 \tTraining Loss: 0.026930\n",
            "Epoch: 30 \tTraining Loss: 0.024121\n",
            "Epoch: 31 \tTraining Loss: 0.027069\n",
            "Epoch: 32 \tTraining Loss: 0.026515\n",
            "Epoch: 33 \tTraining Loss: 0.025372\n",
            "Epoch: 34 \tTraining Loss: 0.028829\n",
            "Epoch: 35 \tTraining Loss: 0.042942\n",
            "Epoch: 36 \tTraining Loss: 0.035310\n",
            "Epoch: 37 \tTraining Loss: 0.026016\n",
            "Epoch: 38 \tTraining Loss: 0.024998\n",
            "Epoch: 39 \tTraining Loss: 0.017868\n",
            "Epoch: 40 \tTraining Loss: 0.020929\n",
            "Epoch: 41 \tTraining Loss: 0.025387\n",
            "Epoch: 42 \tTraining Loss: 0.023030\n",
            "Epoch: 43 \tTraining Loss: 0.020664\n",
            "Epoch: 44 \tTraining Loss: 0.020173\n",
            "Epoch: 45 \tTraining Loss: 0.033893\n",
            "Epoch: 46 \tTraining Loss: 0.031871\n",
            "Epoch: 47 \tTraining Loss: 0.049527\n",
            "Epoch: 48 \tTraining Loss: 0.044580\n",
            "Epoch: 49 \tTraining Loss: 0.039237\n",
            "Epoch: 50 \tTraining Loss: 0.030360\n",
            "Epoch: 51 \tTraining Loss: 0.022797\n",
            "Epoch: 52 \tTraining Loss: 0.018786\n",
            "Epoch: 53 \tTraining Loss: 0.018862\n",
            "Epoch: 54 \tTraining Loss: 0.029158\n",
            "Epoch: 55 \tTraining Loss: 0.018235\n",
            "Epoch: 56 \tTraining Loss: 0.015016\n",
            "Epoch: 57 \tTraining Loss: 0.013432\n",
            "Epoch: 58 \tTraining Loss: 0.013215\n",
            "Epoch: 59 \tTraining Loss: 0.010244\n",
            "Epoch: 60 \tTraining Loss: 0.018858\n",
            "Epoch: 61 \tTraining Loss: 0.020155\n",
            "Epoch: 62 \tTraining Loss: 0.017187\n",
            "Epoch: 63 \tTraining Loss: 0.014112\n",
            "Epoch: 64 \tTraining Loss: 0.011920\n",
            "Epoch: 65 \tTraining Loss: 0.012680\n",
            "Epoch: 66 \tTraining Loss: 0.008512\n",
            "Epoch: 67 \tTraining Loss: 0.009745\n",
            "Epoch: 68 \tTraining Loss: 0.010781\n",
            "Epoch: 69 \tTraining Loss: 0.008302\n",
            "Epoch: 70 \tTraining Loss: 0.008618\n",
            "Epoch: 71 \tTraining Loss: 0.007099\n",
            "Epoch: 72 \tTraining Loss: 0.006784\n",
            "Epoch: 73 \tTraining Loss: 0.007213\n",
            "Epoch: 74 \tTraining Loss: 0.007307\n",
            "Epoch: 75 \tTraining Loss: 0.006411\n",
            "Epoch: 76 \tTraining Loss: 0.006108\n",
            "Epoch: 77 \tTraining Loss: 0.007227\n",
            "Epoch: 78 \tTraining Loss: 0.011354\n",
            "Epoch: 79 \tTraining Loss: 0.010807\n",
            "Epoch: 80 \tTraining Loss: 0.012529\n",
            "Epoch: 81 \tTraining Loss: 0.030158\n",
            "Epoch: 82 \tTraining Loss: 0.023591\n",
            "Epoch: 83 \tTraining Loss: 0.019334\n",
            "Epoch: 84 \tTraining Loss: 0.016591\n",
            "Epoch: 85 \tTraining Loss: 0.012906\n",
            "Epoch: 86 \tTraining Loss: 0.009573\n",
            "Epoch: 87 \tTraining Loss: 0.007397\n",
            "Epoch: 88 \tTraining Loss: 0.009880\n",
            "Epoch: 89 \tTraining Loss: 0.008034\n",
            "Epoch: 90 \tTraining Loss: 0.014263\n",
            "Epoch: 91 \tTraining Loss: 0.014136\n",
            "Epoch: 92 \tTraining Loss: 0.009726\n",
            "Epoch: 93 \tTraining Loss: 0.014357\n",
            "Epoch: 94 \tTraining Loss: 0.016793\n",
            "Epoch: 95 \tTraining Loss: 0.011155\n",
            "Epoch: 96 \tTraining Loss: 0.010258\n",
            "Epoch: 97 \tTraining Loss: 0.007706\n",
            "Epoch: 98 \tTraining Loss: 0.009174\n",
            "Epoch: 99 \tTraining Loss: 0.009283\n",
            "Epoch: 100 \tTraining Loss: 0.010321\n",
            "Epoch: 101 \tTraining Loss: 0.012386\n",
            "Epoch: 102 \tTraining Loss: 0.005487\n",
            "Epoch: 103 \tTraining Loss: 0.006238\n",
            "Epoch: 104 \tTraining Loss: 0.006573\n",
            "Epoch: 105 \tTraining Loss: 0.004389\n",
            "Epoch: 106 \tTraining Loss: 0.004723\n",
            "Epoch: 107 \tTraining Loss: 0.007010\n",
            "Epoch: 108 \tTraining Loss: 0.006142\n",
            "Epoch: 109 \tTraining Loss: 0.005039\n",
            "Epoch: 110 \tTraining Loss: 0.004280\n",
            "Epoch: 111 \tTraining Loss: 0.004589\n",
            "Epoch: 112 \tTraining Loss: 0.005674\n",
            "Epoch: 113 \tTraining Loss: 0.004199\n",
            "Epoch: 114 \tTraining Loss: 0.004481\n",
            "Epoch: 115 \tTraining Loss: 0.005860\n",
            "Epoch: 116 \tTraining Loss: 0.005117\n",
            "Epoch: 117 \tTraining Loss: 0.008142\n",
            "Epoch: 118 \tTraining Loss: 0.005861\n",
            "Epoch: 119 \tTraining Loss: 0.003312\n",
            "Epoch: 120 \tTraining Loss: 0.004024\n",
            "Epoch: 121 \tTraining Loss: 0.004645\n",
            "Epoch: 122 \tTraining Loss: 0.003911\n",
            "Epoch: 123 \tTraining Loss: 0.004586\n",
            "Epoch: 124 \tTraining Loss: 0.003603\n",
            "Epoch: 125 \tTraining Loss: 0.003697\n",
            "Epoch: 126 \tTraining Loss: 0.003531\n",
            "Epoch: 127 \tTraining Loss: 0.007725\n",
            "Epoch: 128 \tTraining Loss: 0.006599\n",
            "Epoch: 129 \tTraining Loss: 0.008613\n",
            "Epoch: 130 \tTraining Loss: 0.007508\n",
            "Epoch: 131 \tTraining Loss: 0.008456\n",
            "Epoch: 132 \tTraining Loss: 0.009723\n",
            "Epoch: 133 \tTraining Loss: 0.009850\n",
            "Epoch: 134 \tTraining Loss: 0.007010\n",
            "Epoch: 135 \tTraining Loss: 0.005072\n",
            "Epoch: 136 \tTraining Loss: 0.005326\n",
            "Epoch: 137 \tTraining Loss: 0.004670\n",
            "Epoch: 138 \tTraining Loss: 0.004497\n",
            "Epoch: 139 \tTraining Loss: 0.005684\n",
            "Epoch: 140 \tTraining Loss: 0.006183\n",
            "Epoch: 141 \tTraining Loss: 0.005207\n",
            "Epoch: 142 \tTraining Loss: 0.008674\n",
            "Epoch: 143 \tTraining Loss: 0.012281\n",
            "Epoch: 144 \tTraining Loss: 0.006029\n",
            "Epoch: 145 \tTraining Loss: 0.006871\n",
            "Epoch: 146 \tTraining Loss: 0.006875\n",
            "Epoch: 147 \tTraining Loss: 0.005492\n",
            "Epoch: 148 \tTraining Loss: 0.005707\n",
            "Epoch: 149 \tTraining Loss: 0.004377\n",
            "Epoch: 150 \tTraining Loss: 0.005059\n",
            "Epoch: 151 \tTraining Loss: 0.005008\n",
            "Epoch: 152 \tTraining Loss: 0.005034\n",
            "Epoch: 153 \tTraining Loss: 0.003639\n",
            "Epoch: 154 \tTraining Loss: 0.006500\n",
            "Epoch: 155 \tTraining Loss: 0.003547\n",
            "Epoch: 156 \tTraining Loss: 0.007524\n",
            "Epoch: 157 \tTraining Loss: 0.006757\n",
            "Epoch: 158 \tTraining Loss: 0.005336\n",
            "Epoch: 159 \tTraining Loss: 0.004656\n",
            "Epoch: 160 \tTraining Loss: 0.005895\n",
            "Epoch: 161 \tTraining Loss: 0.003923\n",
            "Epoch: 162 \tTraining Loss: 0.005827\n",
            "Epoch: 163 \tTraining Loss: 0.004625\n",
            "Epoch: 164 \tTraining Loss: 0.007592\n",
            "Epoch: 165 \tTraining Loss: 0.005960\n",
            "Epoch: 166 \tTraining Loss: 0.004251\n",
            "Epoch: 167 \tTraining Loss: 0.003998\n",
            "Epoch: 168 \tTraining Loss: 0.005151\n",
            "Epoch: 169 \tTraining Loss: 0.004883\n",
            "Epoch: 170 \tTraining Loss: 0.003551\n",
            "Epoch: 171 \tTraining Loss: 0.007025\n",
            "Epoch: 172 \tTraining Loss: 0.004178\n",
            "Epoch: 173 \tTraining Loss: 0.003185\n",
            "Epoch: 174 \tTraining Loss: 0.002966\n",
            "Epoch: 175 \tTraining Loss: 0.003084\n",
            "Epoch: 176 \tTraining Loss: 0.001888\n",
            "Epoch: 177 \tTraining Loss: 0.001868\n",
            "Epoch: 178 \tTraining Loss: 0.004014\n",
            "Epoch: 179 \tTraining Loss: 0.003089\n",
            "Epoch: 180 \tTraining Loss: 0.003040\n",
            "Epoch: 181 \tTraining Loss: 0.004930\n",
            "Epoch: 182 \tTraining Loss: 0.006103\n",
            "Epoch: 183 \tTraining Loss: 0.004831\n",
            "Epoch: 184 \tTraining Loss: 0.002889\n",
            "Epoch: 185 \tTraining Loss: 0.002597\n",
            "Epoch: 186 \tTraining Loss: 0.003607\n",
            "Epoch: 187 \tTraining Loss: 0.002565\n",
            "Epoch: 188 \tTraining Loss: 0.003785\n",
            "Epoch: 189 \tTraining Loss: 0.001996\n",
            "Epoch: 190 \tTraining Loss: 0.002602\n",
            "Epoch: 191 \tTraining Loss: 0.008625\n",
            "Epoch: 192 \tTraining Loss: 0.005145\n",
            "Epoch: 193 \tTraining Loss: 0.007003\n",
            "Epoch: 194 \tTraining Loss: 0.010421\n",
            "Epoch: 195 \tTraining Loss: 0.004960\n",
            "Epoch: 196 \tTraining Loss: 0.006495\n",
            "Epoch: 197 \tTraining Loss: 0.005194\n",
            "Epoch: 198 \tTraining Loss: 0.004307\n",
            "Epoch: 199 \tTraining Loss: 0.005968\n",
            "Epoch: 200 \tTraining Loss: 0.007476\n"
          ]
        }
      ],
      "source": [
        "BiLSTM_model = BiLSTM(vocab_size=len(word_idx),\n",
        "                      embedding_dim=100,\n",
        "                      output_dim=128,\n",
        "                      hidden_dim=256,\n",
        "                      lstm_layers=1,\n",
        "                      bidirectional=True,\n",
        "                      dropout_val=0.33,\n",
        "                      num_classes=len(label_dict))\n",
        "BiLSTM_model.to(device)\n",
        "print(BiLSTM_model)\n",
        "\n",
        "BiLSTM_train = BiLSTM_DataLoader(train_x_vec, train_y_vec)\n",
        "custom_collator = data_collator(word_idx, label_dict)\n",
        "dataloader = DataLoader(dataset=BiLSTM_train,\n",
        "                        batch_size=4,\n",
        "                        drop_last=True,\n",
        "                        collate_fn=custom_collator)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
        "criterion = criterion.to(device)\n",
        "criterion.requres_grad = True\n",
        "optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)\n",
        "epochs = 200\n",
        "#epochs = 5\n",
        "\n",
        "for i in range(1, epochs+1):\n",
        "    train_loss = 0.0\n",
        "    for input, label, input_len, label_len in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = BiLSTM_model(input.to(device), input_len)\n",
        "        output = output.view(-1, len(label_dict))\n",
        "        label = label.view(-1)\n",
        "        loss = criterion(output, label.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * input.size(1)\n",
        "\n",
        "    train_loss = train_loss / len(dataloader.dataset)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
        "    torch.save(BiLSTM_model.state_dict(),\n",
        "               'BiLSTM_b1_epoch_' + str(i) + '.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW9IUurO375G"
      },
      "outputs": [],
      "source": [
        "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
        "custom_collator = data_collator(word_idx, label_dict)\n",
        "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
        "                            batch_size=1,\n",
        "                            shuffle=False,\n",
        "                            drop_last=True,\n",
        "                            collate_fn=custom_collator)\n",
        "for e in range(1,epochs + 1):\n",
        "    BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_b1_epoch_\"+str(e)+\".pt\"))#125\n",
        "    BiLSTM_model.to(device)\n",
        "\n",
        "    \n",
        "    #print(label_dict)\n",
        "    rev_label_dict = {v: k for k, v in label_dict.items()}\n",
        "    rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    file = open(\"./dev1.out\", 'w')\n",
        "    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
        "\n",
        "        pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
        "        pred = pred.cpu()\n",
        "        pred = pred.detach().numpy()\n",
        "        label = label.detach().numpy()\n",
        "        dev_data = dev_data.detach().numpy()\n",
        "        pred = np.argmax(pred, axis=2)\n",
        "        pred = pred.reshape((len(label), -1))\n",
        "\n",
        "        for i in range(len(dev_data)):\n",
        "            for j in range(len(dev_data[i])):\n",
        "                if dev_data[i][j] != 0:\n",
        "                    word = rev_vocab_dict[dev_data[i][j]]\n",
        "                    gold = rev_label_dict[label[i][j]]\n",
        "                    op = rev_label_dict[pred[i][j]]\n",
        "                    file.write(\" \".join([str(j+1), word, gold, op]))\n",
        "                    file.write(\"\\n\")\n",
        "            file.write(\"\\n\")\n",
        "    file.close()\n",
        "    #!perl conll03eval.txt < dev1.out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!perl conll03eval.txt < dev1.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp72PXGw3GX4",
        "outputId": "59602c03-e35a-497e-d9fd-7718750049ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 5712 phrases; correct: 4418.\n",
            "accuracy:  95.10%; precision:  77.35%; recall:  74.35%; FB1:  75.82\n",
            "              LOC: precision:  84.42%; recall:  81.44%; FB1:  82.90  1772\n",
            "             MISC: precision:  81.43%; recall:  75.60%; FB1:  78.40  856\n",
            "              ORG: precision:  66.25%; recall:  71.29%; FB1:  68.68  1443\n",
            "              PER: precision:  77.33%; recall:  68.89%; FB1:  72.87  1641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tezbOS3y375G"
      },
      "outputs": [],
      "source": [
        "#Testing on Test Dataset\n",
        "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
        "custom_test_collator = test_data_collator(word_idx, label_dict)\n",
        "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
        "                                batch_size=1,\n",
        "                                shuffle=False,\n",
        "                                drop_last=True,\n",
        "                                collate_fn=custom_test_collator)\n",
        "for e in range(1,epochs + 1):\n",
        "    BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_b1_epoch_\"+str(e)+\".pt\"))\n",
        "    BiLSTM_model.to(device)\n",
        "\n",
        "    \n",
        "    #print(label_dict)\n",
        "    rev_label_dict = {v: k for k, v in label_dict.items()}\n",
        "    rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    file = open(\"test1.out\", 'w')\n",
        "    for test_data, test_data_len in dataloader_test:\n",
        "\n",
        "        pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
        "        pred = pred.cpu()\n",
        "        pred = pred.detach().numpy()\n",
        "        test_data = test_data.detach().numpy()\n",
        "        pred = np.argmax(pred, axis=2)\n",
        "        pred = pred.reshape((len(test_data), -1))\n",
        "        \n",
        "        for i in range(len(test_data)):\n",
        "            for j in range(len(test_data[i])):\n",
        "                if test_data[i][j] != 0:\n",
        "                    word = rev_vocab_dict[test_data[i][j]]\n",
        "                    op = rev_label_dict[pred[i][j]]\n",
        "                    file.write(\" \".join([str(j+1), word, op]))\n",
        "                    file.write(\"\\n\")\n",
        "\n",
        "            file.write(\"\\n\")        \n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2"
      ],
      "metadata": {
        "id": "ONcGTMQqT8mT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we read the pre-trained GloVe word embeddings from the given file, and create an embedding matrix for the words in the vocabulary of the training, validation, and test data"
      ],
      "metadata": {
        "id": "73aAju1-Uwnp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSh6gqo0375G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c05caf59-79ef-4f43-96b7-aa3f822d719d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30292 100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "glove = pd.read_csv('./glove.6B.100d.txt', sep=\" \",\n",
        "                    quoting=3, header=None, index_col=0)\n",
        "glove_emb = {key: val.values for key, val in glove.T.items()}\n",
        "\n",
        "word_idx = prepare_word_index(train_x, val_x, test_x)\n",
        "glove_vec = np.array([glove_emb[key] for key in glove_emb])\n",
        "glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
        "glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
        "emb_matrix = create_embedding_matrix(word_idx, glove_emb, 100)\n",
        "\n",
        "vocab_size = emb_matrix.shape[0]\n",
        "vector_size = emb_matrix.shape[1]\n",
        "print(vocab_size, vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-YtNevc375G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c42174-fee6-4f24-c4f3-ce7fd8e0d069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BiLSTM_Glove(\n",
            "  (embedding): Embedding(30292, 100)\n",
            "  (LSTM): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (dropout): Dropout(p=0.33, inplace=False)\n",
            "  (elu): ELU(alpha=0.01)\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "Epoch: 1 \tTraining Loss: 0.722542\n",
            "Epoch: 2 \tTraining Loss: 0.338636\n",
            "Epoch: 3 \tTraining Loss: 0.225984\n",
            "Epoch: 4 \tTraining Loss: 0.174482\n",
            "Epoch: 5 \tTraining Loss: 0.137744\n",
            "Epoch: 6 \tTraining Loss: 0.110661\n",
            "Epoch: 7 \tTraining Loss: 0.090044\n",
            "Epoch: 8 \tTraining Loss: 0.075623\n",
            "Epoch: 9 \tTraining Loss: 0.062208\n",
            "Epoch: 10 \tTraining Loss: 0.052554\n",
            "Epoch: 11 \tTraining Loss: 0.044381\n",
            "Epoch: 12 \tTraining Loss: 0.036774\n",
            "Epoch: 13 \tTraining Loss: 0.031368\n",
            "Epoch: 14 \tTraining Loss: 0.027189\n",
            "Epoch: 15 \tTraining Loss: 0.023500\n",
            "Epoch: 16 \tTraining Loss: 0.020209\n",
            "Epoch: 17 \tTraining Loss: 0.018036\n",
            "Epoch: 18 \tTraining Loss: 0.018845\n",
            "Epoch: 19 \tTraining Loss: 0.015091\n",
            "Epoch: 20 \tTraining Loss: 0.012809\n",
            "Epoch: 21 \tTraining Loss: 0.010976\n",
            "Epoch: 22 \tTraining Loss: 0.009390\n",
            "Epoch: 23 \tTraining Loss: 0.009165\n",
            "Epoch: 24 \tTraining Loss: 0.007943\n",
            "Epoch: 25 \tTraining Loss: 0.006949\n",
            "Epoch: 26 \tTraining Loss: 0.006692\n",
            "Epoch: 27 \tTraining Loss: 0.005884\n",
            "Epoch: 28 \tTraining Loss: 0.005608\n",
            "Epoch: 29 \tTraining Loss: 0.005094\n",
            "Epoch: 30 \tTraining Loss: 0.003936\n",
            "Epoch: 31 \tTraining Loss: 0.004302\n",
            "Epoch: 32 \tTraining Loss: 0.003827\n",
            "Epoch: 33 \tTraining Loss: 0.003516\n",
            "Epoch: 34 \tTraining Loss: 0.003389\n",
            "Epoch: 35 \tTraining Loss: 0.004210\n",
            "Epoch: 36 \tTraining Loss: 0.003564\n",
            "Epoch: 37 \tTraining Loss: 0.003183\n",
            "Epoch: 38 \tTraining Loss: 0.002708\n",
            "Epoch: 39 \tTraining Loss: 0.002658\n",
            "Epoch: 40 \tTraining Loss: 0.001935\n",
            "Epoch: 41 \tTraining Loss: 0.002445\n",
            "Epoch: 42 \tTraining Loss: 0.002103\n",
            "Epoch: 43 \tTraining Loss: 0.001696\n",
            "Epoch: 44 \tTraining Loss: 0.002068\n",
            "Epoch: 45 \tTraining Loss: 0.001746\n",
            "Epoch: 46 \tTraining Loss: 0.001859\n",
            "Epoch: 47 \tTraining Loss: 0.002569\n",
            "Epoch: 48 \tTraining Loss: 0.002552\n",
            "Epoch: 49 \tTraining Loss: 0.001353\n",
            "Epoch: 50 \tTraining Loss: 0.001575\n"
          ]
        }
      ],
      "source": [
        "BiLSTM_model = BiLSTM_Glove(vocab_size=len(word_idx),\n",
        "                      embedding_dim=100,\n",
        "                      output_dim=128,\n",
        "                      hidden_dim=256,\n",
        "                      lstm_layers=1,\n",
        "                      bidirectional=True,\n",
        "                      dropout_val=0.33,\n",
        "                      num_classes=len(label_dict),\n",
        "                      emb_matrix=emb_matrix)\n",
        "BiLSTM_model.to(device)\n",
        "print(BiLSTM_model)\n",
        "\n",
        "BiLSTM_train = BiLSTM_DataLoader(train_x_vec, train_y_vec)\n",
        "custom_collator = data_collator(word_idx, label_dict)\n",
        "dataloader = DataLoader(dataset=BiLSTM_train,\n",
        "                        batch_size= 8,\n",
        "                        drop_last=True,\n",
        "                        collate_fn=custom_collator)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
        "criterion = criterion.to(device)\n",
        "criterion.requres_grad = True\n",
        "optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=15, gamma=0.9)\n",
        "epochs = 50\n",
        "\n",
        "for i in range(1, epochs+1):\n",
        "    train_loss = 0.0\n",
        "    for input, label, input_len, label_len in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = BiLSTM_model(input.to(device), input_len)\n",
        "        output = output.view(-1, len(label_dict))\n",
        "        label = label.view(-1)\n",
        "        loss = criterion(output, label.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * input.size(1)\n",
        "\n",
        "    train_loss = train_loss / len(dataloader.dataset)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
        "    torch.save(BiLSTM_model.state_dict(),\n",
        "               'BiLSTM_glove_' + str(i) + '.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thP_iQdq375H"
      },
      "outputs": [],
      "source": [
        "#predicting for validation dataset\n",
        "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
        "custom_collator = data_collator(word_idx, label_dict)\n",
        "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
        "                            batch_size=1,\n",
        "                            shuffle=False,\n",
        "                            drop_last=True,\n",
        "                            collate_fn=custom_collator)\n",
        "for e in range(1, 51):\n",
        "    BiLSTM_model = BiLSTM_Glove(vocab_size=len(word_idx),\n",
        "                        embedding_dim=100,\n",
        "                        output_dim=128,\n",
        "                        hidden_dim=256,\n",
        "                        lstm_layers=1,\n",
        "                        bidirectional=True,\n",
        "                        dropout_val=0.33,\n",
        "                        num_classes=len(label_dict),\n",
        "                        emb_matrix = emb_matrix)\n",
        "\n",
        "    BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_glove_\"+str(e)+\".pt\"))\n",
        "    BiLSTM_model.to(device)\n",
        "    rev_label_dict = {v: k for k, v in label_dict.items()}\n",
        "    rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
        "    \n",
        "    file = open(\"dev2.out\", 'w')\n",
        "    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
        "\n",
        "        pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
        "        pred = pred.cpu()\n",
        "        pred = pred.detach().numpy()\n",
        "        label = label.detach().numpy()\n",
        "        dev_data = dev_data.detach().numpy()\n",
        "        pred = np.argmax(pred, axis=2)\n",
        "        pred = pred.reshape((len(label), -1))\n",
        "\n",
        "        for i in range(len(dev_data)):\n",
        "            for j in range(len(dev_data[i])):\n",
        "                if dev_data[i][j] != 0:\n",
        "                    word = rev_vocab_dict[dev_data[i][j]]\n",
        "                    gold = rev_label_dict[label[i][j]]\n",
        "                    op = rev_label_dict[pred[i][j]]\n",
        "                    file.write(\" \".join([str(j+1), word, gold, op]))\n",
        "                    file.write(\"\\n\")\n",
        "            file.write(\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "    #!perl conll03eval.txt < dev2.out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!perl conll03eval.txt < dev2.out"
      ],
      "metadata": {
        "id": "HPl0l7EY42h_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca9b6e0-4e47-471a-903e-2c2a4dffde23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51578 tokens with 5942 phrases; found: 6054 phrases; correct: 5357.\n",
            "accuracy:  98.04%; precision:  88.49%; recall:  90.15%; FB1:  89.31\n",
            "              LOC: precision:  93.32%; recall:  94.28%; FB1:  93.80  1856\n",
            "             MISC: precision:  81.82%; recall:  83.95%; FB1:  82.87  946\n",
            "              ORG: precision:  82.52%; recall:  84.86%; FB1:  83.68  1379\n",
            "              PER: precision:  91.46%; recall:  93.00%; FB1:  92.22  1873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQyj_MYR375H"
      },
      "outputs": [],
      "source": [
        "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
        "custom_test_collator = test_data_collator(word_idx, label_dict)\n",
        "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
        "                                batch_size=1,\n",
        "                                shuffle=False,\n",
        "                                drop_last=True,\n",
        "                                collate_fn=custom_test_collator)\n",
        "\n",
        "epochs = 50\n",
        "for e in range(1,epochs+1 ):\n",
        "    BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_glove_\"+str(e)+\".pt\"))\n",
        "    BiLSTM_model.to(device)\n",
        "    rev_label_dict = {v: k for k, v in label_dict.items()}\n",
        "    rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
        "\n",
        "    file = open(\"test2.out\", 'w')\n",
        "    for test_data, test_data_len in dataloader_test:\n",
        "\n",
        "        pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
        "        pred = pred.cpu()\n",
        "        pred = pred.detach().numpy()\n",
        "        test_data = test_data.detach().numpy()\n",
        "        pred = np.argmax(pred, axis=2)\n",
        "        pred = pred.reshape((len(test_data), -1))\n",
        "        \n",
        "        for i in range(len(test_data)):\n",
        "            for j in range(len(test_data[i])):\n",
        "                if test_data[i][j] != 0:\n",
        "                    word = rev_vocab_dict[test_data[i][j]]\n",
        "                    op = rev_label_dict[pred[i][j]]\n",
        "                    file.write(\" \".join([str(j+1), word, op]))\n",
        "                    file.write(\"\\n\")\n",
        "\n",
        "            file.write(\"\\n\")        \n",
        "    file.close()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}